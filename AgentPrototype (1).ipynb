{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W8_MZz9HUqxp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bbc5d5e0-883a-495f-9edd-7852113e9175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'source': 'diabetes.csv (Pima Indians Diabetes Database)', 'description': 'Medical diagnostic dataset containing health measurements from 768 Pima Indian women aged 21 and older, collected to predict the onset of diabetes mellitus within 5 years. Originally from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK), part of a long-term epidemiological study begun in 1965 near Phoenix, Arizona.', 'sample description': 'Binary classification problem predicting diabetes diagnosis (0=no diabetes, 1=diabetes) based on 8 diagnostic measurements including glucose levels, blood pressure, BMI, insulin, and other health indicators. The Pima population was studied due to unusually high incidence rates of Type 2 diabetes.', 'notable_previous_work': 'Benchmark dataset widely used in machine learning research since 1988 when included in UCI Machine Learning Repository. Notable research includes: Smith et al. (1988) \"Using the ADAP learning algorithm to forecast the onset of diabetes mellitus\"; hundreds of subsequent studies achieving 75-98% accuracy using various ML algorithms including Random Forest, SVM, Neural Networks, and Deep Learning approaches. Research has explored interpretable models, IoMT applications, and ensemble methods.'}, 'headers': ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from typing import Dict, List, Any\n",
        "import os\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "\n",
        "# Read CSV and get headers - replace data.csv with some given csv\n",
        "df = pd.read_csv('/content/diabetes.csv')\n",
        "headers = df.columns.tolist()\n",
        "\n",
        "# Example metadata - description of how data was accomplished\n",
        "#expecting inclusion of problem to solve, patient statistics, previous work if applicable\n",
        "\n",
        "#to add - category of data(e.g. time-series, nnumerical, categorical, text)\n",
        "#specificed tools\n",
        "output = {\n",
        "    'metadata': {\n",
        "        'source': 'diabetes.csv (Pima Indians Diabetes Database)',\n",
        "        'description': 'Medical diagnostic dataset containing health measurements from 768 Pima Indian women aged 21 and older, collected to predict the onset of diabetes mellitus within 5 years. Originally from the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK), part of a long-term epidemiological study begun in 1965 near Phoenix, Arizona.',\n",
        "        'sample description': 'Binary classification problem predicting diabetes diagnosis (0=no diabetes, 1=diabetes) based on 8 diagnostic measurements including glucose levels, blood pressure, BMI, insulin, and other health indicators. The Pima population was studied due to unusually high incidence rates of Type 2 diabetes.',\n",
        "        'notable_previous_work': 'Benchmark dataset widely used in machine learning research since 1988 when included in UCI Machine Learning Repository. Notable research includes: Smith et al. (1988) \"Using the ADAP learning algorithm to forecast the onset of diabetes mellitus\"; hundreds of subsequent studies achieving 75-98% accuracy using various ML algorithms including Random Forest, SVM, Neural Networks, and Deep Learning approaches. Research has explored interpretable models, IoMT applications, and ensemble methods.'\n",
        "    },\n",
        "    'headers': headers  # Just the list of column names\n",
        "}\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "wKe2_Cw7elUQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SimpleFeaturePipeline:\n",
        "    def __init__(self, api_key, serper_api_key=None, task_type='classification'):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.serper_api_key = serper_api_key\n",
        "        self.task_type = task_type\n",
        "        self.history = []\n",
        "\n",
        "    def run(self, df, target_col, max_iterations=30, min_improvement=0.01, patience=2, metadata=None):\n",
        "        # ^^^ ADD metadata=None here\n",
        "        X = df.drop(columns=[target_col])\n",
        "        y = df[target_col]\n",
        "        best_score = 0\n",
        "        no_improvement_count = 0\n",
        "\n",
        "        print(f\"Starting pipeline: {X.shape[0]} rows, {X.shape[1]} features\")\n",
        "\n",
        "        for iteration in range(1, max_iterations + 1):\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"ITERATION {iteration}\")\n",
        "            print(f\"{'='*60}\")\n",
        "\n",
        "            print(\"Agent: Researching features...\")\n",
        "            feedback = self.history[-1]['feedback'] if self.history else None\n",
        "            new_features = self._agent_research(X, target_col, feedback, metadata)\n",
        "\n",
        "            print(\"Agent: Generating features...\")\n",
        "            X_augmented = self._generate_features(X, new_features)\n",
        "\n",
        "            print(\"Evaluator: Testing features...\")\n",
        "            score, top_features = self._evaluate(X_augmented, y)\n",
        "\n",
        "            self.history.append({\n",
        "                'iteration': iteration,\n",
        "                'score': score,\n",
        "                'num_features': X_augmented.shape[1],\n",
        "                'top_features': top_features,\n",
        "                'feedback': {'best_score': score, 'top_features': top_features}\n",
        "            })\n",
        "\n",
        "            print(f\"Score: {score:.4f} | Features: {X_augmented.shape[1]}\")\n",
        "\n",
        "            improvement = score - best_score\n",
        "\n",
        "            if improvement > min_improvement:\n",
        "                print(f\"✓ Improvement: +{improvement:.4f}\")\n",
        "                best_score = score\n",
        "                no_improvement_count = 0\n",
        "                X = X_augmented\n",
        "            else:\n",
        "                print(f\"✗ No significant improvement (+{improvement:.4f})\")\n",
        "                no_improvement_count += 1\n",
        "\n",
        "                if no_improvement_count >= patience:\n",
        "                    print(f\"\\n⚠ EARLY STOPPING: No improvement for {patience} iterations\")\n",
        "                    break\n",
        "\n",
        "                X = X_augmented\n",
        "\n",
        "        best = max(self.history, key=lambda x: x['score'])\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"BEST: Iteration {best['iteration']} - Score: {best['score']:.4f}\")\n",
        "        print(f\"Total iterations: {len(self.history)}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        return X, best\n",
        "\n",
        "    def _evaluate(self, X, y):\n",
        "     \"\"\"Evaluate features with simple model\"\"\"\n",
        "     from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "     from sklearn.model_selection import cross_val_score\n",
        "     import numpy as np\n",
        "\n",
        "     # Clean the data before evaluation\n",
        "     X_cleaned = X.copy()\n",
        "\n",
        "     # Replace infinity with NaN\n",
        "     X_cleaned = X_cleaned.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "     # Fill NaN with column mean\n",
        "     X_cleaned = X_cleaned.fillna(X_cleaned.mean())\n",
        "\n",
        "     # If still has NaN (column was all NaN), fill with 0\n",
        "     X_cleaned = X_cleaned.fillna(0)\n",
        "\n",
        "     # Check for any remaining issues\n",
        "     if not np.isfinite(X_cleaned.values).all():\n",
        "         print(\"  ⚠ Warning: Non-finite values detected after cleaning\")\n",
        "         # Drop columns with non-finite values\n",
        "         bad_cols = X_cleaned.columns[~np.isfinite(X_cleaned).all()].tolist()\n",
        "         print(f\"  ⚠ Dropping problematic columns: {bad_cols}\")\n",
        "         X_cleaned = X_cleaned.drop(columns=bad_cols)\n",
        "\n",
        "     # Choose model\n",
        "     if self.task_type == 'classification':\n",
        "         model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "     else:\n",
        "         model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "\n",
        "     try:\n",
        "         # Cross-validation score\n",
        "         scores = cross_val_score(model, X_cleaned, y, cv=3)\n",
        "         avg_score = scores.mean()\n",
        "\n",
        "         # Feature importance\n",
        "         model.fit(X_cleaned, y)\n",
        "         importances = dict(zip(X_cleaned.columns, model.feature_importances_))\n",
        "         top_features = sorted(importances.items(), key=lambda x: -x[1])[:10]\n",
        "         top_features = [f[0] for f in top_features]\n",
        "\n",
        "         return avg_score, top_features\n",
        "\n",
        "     except Exception as e:\n",
        "         print(f\"  ⚠ Evaluation failed: {e}\")\n",
        "         # Return poor score if evaluation fails\n",
        "         return 0.0, list(X_cleaned.columns[:5])\n",
        "\n",
        "\n",
        "    def _generate_features(self, X, agent_result):\n",
        "      \"\"\"Generate features based on agent actions with safety checks\"\"\"\n",
        "      import numpy as np\n",
        "\n",
        "      X_new = X.copy()\n",
        "      kept_features = set()\n",
        "\n",
        "      print(\"\\n  🔧 Executing feature actions:\")\n",
        "\n",
        "      # First pass: Handle KEEP and CREATE\n",
        "      for action in agent_result.get('actions', []):\n",
        "          action_type = action['action']\n",
        "          name = action['name']\n",
        "\n",
        "          if action_type == 'keep':\n",
        "              if name in X_new.columns:\n",
        "                  kept_features.add(name)\n",
        "                  print(f\"    ✓ Kept: {name}\")\n",
        "              else:\n",
        "                  print(f\"    ✗ Cannot keep '{name}' - not found\")\n",
        "\n",
        "          elif action_type == 'create':\n",
        "              formula = action['formula']\n",
        "              try:\n",
        "                  namespace = {\"__builtins__\": {}, \"np\": np}\n",
        "                  namespace.update(X_new.to_dict('series'))\n",
        "\n",
        "                  result = eval(formula, namespace)\n",
        "\n",
        "                  # Safety checks\n",
        "                  if isinstance(result, pd.Series):\n",
        "                      # Check for infinity\n",
        "                      if np.isinf(result).any():\n",
        "                          print(f\"    ⚠ '{name}' contains infinity - clipping values\")\n",
        "                          result = result.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "                      # Check for too many NaN\n",
        "                      nan_pct = result.isna().sum() / len(result)\n",
        "                      if nan_pct > 0.5:\n",
        "                          print(f\"    ⚠ '{name}' has {nan_pct:.1%} NaN - skipping\")\n",
        "                          continue\n",
        "\n",
        "                      # Check for constant values\n",
        "                      if result.nunique() == 1:\n",
        "                          print(f\"    ⚠ '{name}' is constant - skipping\")\n",
        "                          continue\n",
        "\n",
        "                      X_new[name] = result\n",
        "                      print(f\"    ✓ Created: {name} = {formula}\")\n",
        "                  else:\n",
        "                      print(f\"    ✗ '{name}' didn't produce a Series\")\n",
        "\n",
        "              except ZeroDivisionError:\n",
        "                  print(f\"    ✗ Failed '{name}': Division by zero\")\n",
        "              except Exception as e:\n",
        "                  print(f\"    ✗ Failed '{name}': {str(e)[:50]}\")\n",
        "\n",
        "      # Second pass: Handle REMOVE\n",
        "      for action in agent_result.get('actions', []):\n",
        "          if action['action'] == 'remove':\n",
        "              name = action['name']\n",
        "              if name in X_new.columns and name not in kept_features:\n",
        "                  X_new = X_new.drop(columns=[name])\n",
        "                  print(f\"    ✓ Removed: {name}\")\n",
        "\n",
        "      # Keep all features that weren't explicitly removed\n",
        "      original_kept = set(X.columns) - {a['name'] for a in agent_result.get('actions', []) if a['action'] == 'remove'}\n",
        "      for col in original_kept:\n",
        "          if col not in X_new.columns:\n",
        "              X_new[col] = X[col]\n",
        "\n",
        "      # Final safety check on all columns\n",
        "      print(\"\\n  🔍 Final data quality check:\")\n",
        "      problematic_cols = []\n",
        "      for col in X_new.columns:\n",
        "          if np.isinf(X_new[col]).any():\n",
        "              problematic_cols.append(col)\n",
        "              print(f\"    ⚠ '{col}' has infinity values - will be cleaned in evaluation\")\n",
        "          elif X_new[col].isna().all():\n",
        "              problematic_cols.append(col)\n",
        "              print(f\"    ⚠ '{col}' is all NaN - dropping\")\n",
        "              X_new = X_new.drop(columns=[col])\n",
        "\n",
        "      print(f\"\\n  📊 Result: {len(X.columns)} → {len(X_new.columns)} features\")\n",
        "      if problematic_cols:\n",
        "          print(f\"  ⚠ {len(problematic_cols)} features need cleaning\")\n",
        "\n",
        "      return X_new\n",
        "\n",
        "    def _web_search(self, query):\n",
        "        if not self.serper_api_key:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            import requests\n",
        "            url = \"https://google.serper.dev/search\"\n",
        "            payload = {\"q\": query, \"num\": 5}\n",
        "            headers = {\n",
        "                \"X-API-KEY\": self.serper_api_key,\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "\n",
        "            response = requests.post(url, json=payload, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            results = response.json()\n",
        "\n",
        "            snippets = []\n",
        "            for item in results.get('organic', [])[:3]:\n",
        "                title = item.get('title', '')\n",
        "                snippet = item.get('snippet', '')\n",
        "                snippets.append(f\"**{title}**\\n{snippet}\")\n",
        "\n",
        "            return \"\\n\\n\".join(snippets) if snippets else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠ Search error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _validate_formula(self, formula, X):\n",
        "\n",
        "\n",
        "\n",
        "      if not formula or not isinstance(formula, str):\n",
        "          return False\n",
        "\n",
        "      # Check for dangerous operations\n",
        "      dangerous = ['eval', 'exec', 'import', '__', 'open', 'file', 'os', 'sys']\n",
        "      if any(word in formula.lower() for word in dangerous):\n",
        "          return False\n",
        "\n",
        "      # Try to evaluate formula safely\n",
        "      try:\n",
        "          # Create a small test dataframe\n",
        "          test_df = X.head(10).copy()  # Use more rows for better testing\n",
        "\n",
        "          # Attempt to evaluate\n",
        "          namespace = {\"__builtins__\": {}, \"np\": np}\n",
        "          namespace.update(test_df.to_dict('series'))\n",
        "          result = eval(formula, namespace)\n",
        "\n",
        "          # Check result is valid\n",
        "          if not (hasattr(result, '__len__') or isinstance(result, (int, float))):\n",
        "              return False\n",
        "\n",
        "          # Check for infinity or all NaN\n",
        "          if isinstance(result, pd.Series):\n",
        "              if np.isinf(result).any():\n",
        "                  return False  # Reject formulas that produce infinity\n",
        "              if result.isna().all():\n",
        "                  return False  # Reject formulas that produce all NaN\n",
        "              if result.nunique() == 1:\n",
        "                  return False  # Reject constant features\n",
        "\n",
        "          return True\n",
        "\n",
        "      except ZeroDivisionError:\n",
        "          return False  # Reject division by zero\n",
        "      except Exception as e:\n",
        "          return False\n",
        "\n",
        "    def _perform_research(self, target, metadata, feedback):\n",
        "      print(\"  🔍 Performing web research...\")\n",
        "\n",
        "      domain = metadata.get('domain', 'general')\n",
        "      description = metadata.get('description', '')[:100]\n",
        "\n",
        "      # Extract the actual prediction goal from metadata\n",
        "      # For diabetes: \"predicting diabetes onset\"\n",
        "      # For churn: \"predicting customer churn\"\n",
        "      problem_description = metadata.get('problem', description)\n",
        "\n",
        "      # Better queries that focus on the domain problem, not column names\n",
        "      queries = [\n",
        "          f\"feature engineering {domain} {problem_description}\",\n",
        "          f\"best features for {domain} machine learning prediction\",\n",
        "          f\"{domain} feature interactions data science\"\n",
        "      ]\n",
        "\n",
        "      # Add feedback-informed query\n",
        "      if feedback and feedback.get('top_features'):\n",
        "          top_features_str = ', '.join(feedback['top_features'][:3])\n",
        "          queries.append(f\"{domain} {top_features_str} feature engineering\")\n",
        "\n",
        "      all_findings = []\n",
        "      for query in queries[:3]:  # Limit to 3 queries to save API calls\n",
        "          print(f\"  → Searching: '{query[:60]}...'\")\n",
        "          results = self._web_search(query)\n",
        "\n",
        "          if results:\n",
        "              all_findings.append(f\"Query: {query}\\n{results}\")\n",
        "              print(f\"  ✓ Found {len(results)} chars of results\")\n",
        "          else:\n",
        "              print(f\"  ✗ No results\")\n",
        "\n",
        "          import time\n",
        "          time.sleep(0.5)\n",
        "\n",
        "      if not all_findings:\n",
        "          print(\"  ⚠ No search results - using general knowledge\")\n",
        "          return None\n",
        "\n",
        "      # Synthesize findings with LLM\n",
        "      synthesis_prompt = f\"\"\"Based on these web search results about feature engineering:\n",
        "\n",
        "{chr(10).join(all_findings)}\n",
        "\n",
        "Dataset context:\n",
        "- Domain: {domain}\n",
        "- Problem: {problem_description}\n",
        "- Target variable: {target}\n",
        "- Available columns: {metadata.get('headers', [])}\n",
        "{f\"- Previous best features: {feedback['top_features'][:5]}\" if feedback else \"\"}\n",
        "\n",
        "Extract 3-5 SPECIFIC, ACTIONABLE feature engineering recommendations:\n",
        "1. Use the EXACT column names from 'Available columns' list above\n",
        "2. Suggest concrete formulas (e.g., \"BMI * Age\", \"Glucose / Insulin\")\n",
        "3. Explain WHY based on domain knowledge from search results\n",
        "4. Focus on {domain} domain-specific transformations\"\"\"\n",
        "\n",
        "      response = self.client.chat.completions.create(\n",
        "          model=\"gpt-4o\",\n",
        "          messages=[{\"role\": \"user\", \"content\": synthesis_prompt}]\n",
        "      )\n",
        "\n",
        "      findings = response.choices[0].message.content\n",
        "      print(f\"  ✓ Synthesized research: {len(findings)} chars\")\n",
        "      return findings\n",
        "\n",
        "\n",
        "    def _agent_research(self, X, target, feedback, metadata=None):\n",
        "\n",
        "\n",
        "\n",
        "        research_context = \"\"\n",
        "        if metadata:\n",
        "            research_context = self._perform_research(target, metadata, feedback)\n",
        "\n",
        "        current_features_summary = {\n",
        "            'total': len(X.columns),\n",
        "            'names': X.columns.tolist(),\n",
        "            'sample_stats': {\n",
        "                col: {'mean': X[col].mean(), 'std': X[col].std()}\n",
        "                for col in X.columns[:5]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        prompt = f\"\"\"You're a feature engineering expert with research capabilities.\n",
        "\n",
        "DATASET INFO:\n",
        "- Target: {target}\n",
        "- Current Columns ({len(X.columns)}): {X.columns.tolist()}\n",
        "- Shape: {X.shape}\n",
        "\n",
        "{f'''PREVIOUS ITERATION FEEDBACK:\n",
        "- Score: {feedback['best_score']:.3f}\n",
        "- Top 5 performing features: {feedback['top_features'][:5]}\n",
        "- Strategy: Build on what's working or try complementary approaches\n",
        "''' if feedback else \"FIRST ITERATION: Explore fundamental feature relationships\"}\n",
        "\n",
        "{f\"RESEARCH INSIGHTS:\\n{research_context}\\n\" if research_context else \"\"}\n",
        "\n",
        "TASK: Design a feature engineering strategy. You MUST specify actions for features:\n",
        "\n",
        "ACTIONS YOU CAN TAKE:\n",
        "1. **KEEP** - Preserve existing features that are valuable\n",
        "   - Always keep top-performing features from previous iteration\n",
        "   - Keep features mentioned in research as important\n",
        "\n",
        "2. **CREATE** - Generate new features via transformations\n",
        "   - Interactions: col1 * col2, col1 / col2\n",
        "   - Polynomials: col1 ** 2, np.sqrt(col1)\n",
        "   - Aggregations: (col1 + col2 + col3) / 3\n",
        "   - Domain-specific: Use research insights\n",
        "\n",
        "3. **REMOVE** - Drop features that aren't helping (optional, use sparingly)\n",
        "   - Only remove if clearly redundant or harmful\n",
        "\n",
        "IMPORTANT RULES:\n",
        "- Use EXACT column names from: {X.columns.tolist()}\n",
        "- If previous iteration had good features, include KEEP actions for them\n",
        "- Formulas must be valid Python/pandas (use np.log, np.sqrt, etc.)\n",
        "- Suggest 5-10 total actions\n",
        "- Balance: keep proven features + create promising new ones\n",
        "\n",
        "OUTPUT (JSON only):\n",
        "{{\n",
        "  \"domain\": \"identified domain\",\n",
        "  \"reasoning\": \"strategy explanation based on research and feedback\",\n",
        "  \"actions\": [\n",
        "    {{\n",
        "      \"action\": \"keep\",\n",
        "      \"name\": \"Glucose\",\n",
        "      \"why\": \"Top feature from previous iteration with 0.25 importance\"\n",
        "    }},\n",
        "    {{\n",
        "      \"action\": \"create\",\n",
        "      \"name\": \"BMI_Glucose_interaction\",\n",
        "      \"formula\": \"BMI * Glucose\",\n",
        "      \"why\": \"Research suggests metabolic syndrome indicators interact\"\n",
        "    }},\n",
        "    {{\n",
        "      \"action\": \"create\",\n",
        "      \"name\": \"Age_squared\",\n",
        "      \"formula\": \"Age ** 2\",\n",
        "      \"why\": \"Capture non-linear age effects on diabetes risk\"\n",
        "    }}\n",
        "  ]\n",
        "}}\"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "\n",
        "        result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "        validated_result = self._validate_agent_response(result, X, feedback)\n",
        "\n",
        "        print(f\"\\n  📋 Agent Strategy:\")\n",
        "        print(f\"    Domain: {validated_result.get('domain', 'unknown')}\")\n",
        "        print(f\"    Reasoning: {validated_result.get('reasoning', 'N/A')[:100]}...\")\n",
        "\n",
        "        action_counts = {'keep': 0, 'create': 0, 'remove': 0}\n",
        "        for action in validated_result['actions']:\n",
        "            action_type = action['action']\n",
        "            action_counts[action_type] = action_counts.get(action_type, 0) + 1\n",
        "\n",
        "        print(f\"    Actions: {sum(action_counts.values())} total → Keep: {action_counts['keep']}, Create: {action_counts['create']}, Remove: {action_counts['remove']}\")\n",
        "\n",
        "        return validated_result\n",
        "\n",
        "    def _validate_agent_response(self, result, X, feedback=None):\n",
        "        # ^^^ INDENT THIS TOO!\n",
        "        \"\"\"Validate and sanitize agent suggestions\"\"\"\n",
        "        print(\"  ✓ Validating suggestions...\")\n",
        "\n",
        "        valid_actions = []\n",
        "        existing_cols = set(X.columns)\n",
        "\n",
        "        if feedback and feedback.get('top_features'):\n",
        "            top_features = feedback['top_features'][:3]\n",
        "            suggested_keeps = {a['name'] for a in result.get('actions', []) if a.get('action') == 'keep'}\n",
        "\n",
        "            for feature in top_features:\n",
        "                if feature in existing_cols and feature not in suggested_keeps:\n",
        "                    valid_actions.append({\n",
        "                        \"action\": \"keep\",\n",
        "                        \"name\": feature,\n",
        "                        \"why\": f\"Auto-kept: top feature from previous iteration\"\n",
        "                    })\n",
        "                    print(f\"  ✓ Auto-keeping top feature: {feature}\")\n",
        "\n",
        "        for action in result.get('actions', []):\n",
        "            action_type = action.get('action', '').lower()\n",
        "            name = action.get('name', '')\n",
        "\n",
        "            if action_type == 'keep':\n",
        "                if name in existing_cols:\n",
        "                    if not any(a['action'] == 'keep' and a['name'] == name for a in valid_actions):\n",
        "                        valid_actions.append(action)\n",
        "                        print(f\"  ✓ Keep: {name}\")\n",
        "                else:\n",
        "                    print(f\"  ⚠ Cannot keep '{name}' - not in dataset\")\n",
        "\n",
        "            elif action_type == 'create':\n",
        "                formula = action.get('formula', '')\n",
        "                if self._validate_formula(formula, X):\n",
        "                    valid_actions.append(action)\n",
        "                    print(f\"  ✓ Create: {name} = {formula}\")\n",
        "                else:\n",
        "                    print(f\"  ⚠ Invalid formula for '{name}': {formula}\")\n",
        "\n",
        "            elif action_type == 'remove':\n",
        "                if name in existing_cols:\n",
        "                    if feedback and name in feedback.get('top_features', [])[:5]:\n",
        "                        print(f\"  ⚠ Blocking remove of '{name}' - it's a top feature\")\n",
        "                    else:\n",
        "                        valid_actions.append(action)\n",
        "                        print(f\"  ✓ Remove: {name}\")\n",
        "                else:\n",
        "                    print(f\"  ⚠ Cannot remove '{name}' - not in dataset\")\n",
        "\n",
        "        result['actions'] = valid_actions\n",
        "\n",
        "        if not valid_actions:\n",
        "            print(\"  ⚠ WARNING: No valid actions! Using fallback...\")\n",
        "            result['actions'] = [\n",
        "                {\"action\": \"keep\", \"name\": col, \"why\": \"fallback preservation\"}\n",
        "                for col in X.columns[:5]\n",
        "            ]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _generate_features(self, X, agent_result):\n",
        "\n",
        "      import numpy as np\n",
        "\n",
        "      X_new = X.copy()\n",
        "      kept_features = set()\n",
        "\n",
        "      print(\"\\n  🔧 Executing feature actions:\")\n",
        "\n",
        "      # First pass: Handle KEEP and CREATE\n",
        "      for action in agent_result.get('actions', []):\n",
        "          action_type = action['action']\n",
        "          name = action['name']\n",
        "\n",
        "          if action_type == 'keep':\n",
        "              if name in X_new.columns:\n",
        "                  kept_features.add(name)\n",
        "                  print(f\"    ✓ Kept: {name}\")\n",
        "              else:\n",
        "                  print(f\"    ✗ Cannot keep '{name}' - not found\")\n",
        "\n",
        "          elif action_type == 'create':\n",
        "              formula = action['formula']\n",
        "              try:\n",
        "                  namespace = {\"__builtins__\": {}, \"np\": np}\n",
        "                  namespace.update(X_new.to_dict('series'))\n",
        "\n",
        "                  result = eval(formula, namespace)\n",
        "\n",
        "                  # Safety checks\n",
        "                  if isinstance(result, pd.Series):\n",
        "                      # Check for infinity\n",
        "                      if np.isinf(result).any():\n",
        "                          print(f\"    ⚠ '{name}' contains infinity - clipping values\")\n",
        "                          result = result.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "                      # Check for too many NaN\n",
        "                      nan_pct = result.isna().sum() / len(result)\n",
        "                      if nan_pct > 0.5:\n",
        "                          print(f\"    ⚠ '{name}' has {nan_pct:.1%} NaN - skipping\")\n",
        "                          continue\n",
        "\n",
        "                      # Check for constant values\n",
        "                      if result.nunique() == 1:\n",
        "                          print(f\"    ⚠ '{name}' is constant - skipping\")\n",
        "                          continue\n",
        "\n",
        "                      X_new[name] = result\n",
        "                      print(f\"    ✓ Created: {name} = {formula}\")\n",
        "                  else:\n",
        "                      print(f\"    ✗ '{name}' didn't produce a Series\")\n",
        "\n",
        "              except ZeroDivisionError:\n",
        "                  print(f\"    ✗ Failed '{name}': Division by zero\")\n",
        "              except Exception as e:\n",
        "                  print(f\"    ✗ Failed '{name}': {str(e)[:50]}\")\n",
        "\n",
        "      # Second pass: Handle REMOVE\n",
        "      for action in agent_result.get('actions', []):\n",
        "          if action['action'] == 'remove':\n",
        "              name = action['name']\n",
        "              if name in X_new.columns and name not in kept_features:\n",
        "                  X_new = X_new.drop(columns=[name])\n",
        "                  print(f\"    ✓ Removed: {name}\")\n",
        "\n",
        "      # Keep all features that weren't explicitly removed\n",
        "      original_kept = set(X.columns) - {a['name'] for a in agent_result.get('actions', []) if a['action'] == 'remove'}\n",
        "      for col in original_kept:\n",
        "          if col not in X_new.columns:\n",
        "              X_new[col] = X[col]\n",
        "\n",
        "      # Final safety check on all columns\n",
        "      print(\"\\n  🔍 Final data quality check:\")\n",
        "      problematic_cols = []\n",
        "      for col in X_new.columns:\n",
        "          if np.isinf(X_new[col]).any():\n",
        "              problematic_cols.append(col)\n",
        "              print(f\"    ⚠ '{col}' has infinity values - will be cleaned in evaluation\")\n",
        "          elif X_new[col].isna().all():\n",
        "              problematic_cols.append(col)\n",
        "              print(f\"    ⚠ '{col}' is all NaN - dropping\")\n",
        "              X_new = X_new.drop(columns=[col])\n",
        "\n",
        "      print(f\"\\n  📊 Result: {len(X.columns)} → {len(X_new.columns)} features\")\n",
        "      if problematic_cols:\n",
        "          print(f\"  ⚠ {len(problematic_cols)} features need cleaning\")\n",
        "\n",
        "      return X_new\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    df = pd.read_csv('/content/diabetes.csv')\n",
        "\n",
        "    # Run pipeline\n",
        "    pipeline = SimpleFeaturePipeline(\n",
        "        api_key= userdata.get('openaiapi'),\n",
        "        serper_api_key= userdata.get('serper'),\n",
        "        task_type='classification'  # or 'regression'\n",
        "    )\n",
        "\n",
        "    metadata = {\n",
        "    'domain': 'medical diagnostics',\n",
        "    'problem': 'diabetes prediction',  # ADD THIS - what we're actually predicting\n",
        "    'description': 'Pima Indians Diabetes Dataset for predicting diabetes onset in women',\n",
        "    'headers': df.columns.tolist(),\n",
        "    'notable_previous_work': 'Random Forest achieves 75-80% accuracy'\n",
        "    }\n",
        "\n",
        "    X_best, results = pipeline.run(\n",
        "        df=df,\n",
        "        target_col='Outcome',  # Changed from 'target' to match diabetes dataset\n",
        "        max_iterations=30,\n",
        "        metadata=metadata  # Add metadata\n",
        "    )\n",
        "\n",
        "    # Show results\n",
        "    print(\"\\nTop 10 Features:\")\n",
        "    for i, feat in enumerate(results['top_features'], 1):\n",
        "        print(f\"{i}. {feat}\")\n",
        "\n",
        "    # Save augmented data\n",
        "   # Save augmented data\n",
        "    X_best['Outcome'] = df['Outcome']  # Change 'target' to 'Outcome'\n",
        "    X_best.to_csv('augmented_data.csv', index=False)\n",
        "    print(\"\\nSaved to: augmented_data.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "t2l_b4xuZ0a5",
        "outputId": "0815142d-e780-408d-e10b-080bd48eed4e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting pipeline: 768 rows, 8 features\n",
            "\n",
            "============================================================\n",
            "ITERATION 1\n",
            "============================================================\n",
            "Agent: Researching features...\n",
            "  🔍 Performing web research...\n",
            "  → Searching: 'feature engineering medical diagnostics diabetes prediction...'\n",
            "  ✓ Found 670 chars of results\n",
            "  → Searching: 'best features for medical diagnostics machine learning predi...'\n",
            "  ✓ Found 672 chars of results\n",
            "  → Searching: 'medical diagnostics feature interactions data science...'\n",
            "  ✓ Found 690 chars of results\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1042442042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    623\u001b[0m     } \n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m     X_best, results = pipeline.run(\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Outcome'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Changed from 'target' to match diabetes dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1042442042.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, df, target_col, max_iterations, min_improvement, patience, metadata)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agent: Researching features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feedback'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent_research\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Agent: Generating features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1042442042.py\u001b[0m in \u001b[0;36m_agent_research\u001b[0;34m(self, X, target, feedback, metadata)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mresearch_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mresearch_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_research\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         current_features_summary = {\n",
            "\u001b[0;32m/tmp/ipython-input-1042442042.py\u001b[0m in \u001b[0;36m_perform_research\u001b[0;34m(self, target, metadata, feedback)\u001b[0m\n\u001b[1;32m    335\u001b[0m 4. Focus on {domain} domain-specific transformations\"\"\"\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m       response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m    338\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msynthesis_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nExyZQuwVlFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}