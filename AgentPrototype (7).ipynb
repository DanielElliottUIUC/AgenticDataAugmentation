{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W8_MZz9HUqxp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from typing import Dict, List, Any\n",
        "import os\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "import time, traceback\n",
        "from typing import Any, Dict, List, Optional\n",
        "import math\n",
        "\n",
        "\n",
        "# Suppress deprecation warnings from Jupyter\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# Read CSV and get headers - replace data.csv with some given csv\n",
        "# df = pd.read_csv('/content/diabetes.csv')\n",
        "# headers = df.columns.tolist()\n",
        "\n",
        "# Example metadata - description of how data was accomplished\n",
        "#expecting inclusion of problem to solve, patient statistics, previous work if applicable\n",
        "\n",
        "#to add - category of data(e.g. time-series, nnumerical, categorical, text)\n",
        "#specificed tools\n",
        "\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- put near your imports ----\n",
        "import json, time, traceback\n",
        "from typing import Any, Dict, Optional, List\n",
        "\n",
        "class ResearchFailedError(RuntimeError):\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from pydantic import BaseModel, Field, ValidationError\n",
        "    _HAVE_PYDANTIC = True\n",
        "except Exception:\n",
        "    _HAVE_PYDANTIC = False\n",
        "\n",
        "\n",
        "# ---- minimal strict schema (optional but helpful) ----\n",
        "if _HAVE_PYDANTIC:\n",
        "    class InterpretationGuide(BaseModel):\n",
        "        high_values: Optional[str] = None\n",
        "        low_values: Optional[str] = None\n",
        "\n",
        "    class ExplainableFeature(BaseModel):\n",
        "        feature_concept: str\n",
        "        formula_pattern: Optional[str] = None\n",
        "        physiological_explanation: Optional[str] = None\n",
        "        clinical_relevance: Optional[str] = None\n",
        "        explainability_score: Optional[int] = Field(default=None, ge=1, le=5)\n",
        "        used_by: Optional[str] = None\n",
        "        interpretation_guide: Optional[InterpretationGuide] = None\n",
        "        source: Optional[str] = None\n",
        "\n",
        "    class PhysiologicalResearchResponse(BaseModel):\n",
        "        explainable_features: List[ExplainableFeature] = Field(default_factory=list)\n",
        "        domain_transformations: List[Dict[str, Any]] = Field(default_factory=list)\n",
        "        explainable_interactions: List[Any] = Field(default_factory=list)\n",
        "        features_to_avoid: List[str] = Field(default_factory=list)\n",
        "        insights_summary: Optional[str] = \"\"\n",
        "\n",
        "\n",
        "def _strip_markdown_fences(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        # remove backticks and take substring from the first '{'\n",
        "        s = s.strip(\"`\")\n",
        "        i = s.find(\"{\")\n",
        "        if i >= 0:\n",
        "            s = s[i:]\n",
        "    return s\n",
        "\n",
        "\n",
        "class ResearchAgent:\n",
        "    \"\"\"Web research for physiologically explainable features. Hard-fail on errors (no fallback).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model: str = \"gpt-4o-2024-08-06\",\n",
        "        retries: int = 0,              # 0 = no retry (minimize token use)\n",
        "        backoff_sec: float = 0.6,\n",
        "        require_min_features: int = 1, # fail if fewer than this survive\n",
        "        min_explainability: int = 3    # filter threshold\n",
        "        ):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.retries = max(0, retries)\n",
        "        self.backoff_sec = backoff_sec\n",
        "        self.require_min_features = require_min_features\n",
        "        self.min_explainability = min_explainability\n",
        "\n",
        "    def search(self, target: str, metadata: Dict[str, Any], feedback: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Attempt research once (or with small retry). If it fails -> raise ResearchFailedError.\"\"\"\n",
        "        print(\"  üîç Performing physiologically-focused research with OpenAI...\")\n",
        "\n",
        "        if self.client is None:\n",
        "            raise ResearchFailedError(\"OpenAI client is not configured.\")\n",
        "\n",
        "        # Keep prompt compact to reduce tokens and reduce failure modes.\n",
        "        domain = metadata.get(\"domain\", \"general\")\n",
        "        problem_description = metadata.get(\"problem\", metadata.get(\"description\", \"\"))\n",
        "        cols = metadata.get(\"column_names\", [])\n",
        "        cols_disp = cols[:30]  # cap column list length\n",
        "\n",
        "        research_prompt = self._build_prompt(\n",
        "            domain=domain,\n",
        "            problem=problem_description,\n",
        "            target=target,\n",
        "            cols_disp=cols_disp,\n",
        "            feedback=feedback\n",
        "        )\n",
        "\n",
        "        last_exc = None\n",
        "        attempts = self.retries + 1\n",
        "        for attempt in range(attempts):\n",
        "            try:\n",
        "                completion = self.client.chat.completions.create(\n",
        "                    model=self.model,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": (\n",
        "                                \"You are a medical/biological research analyst specializing in INTERPRETABLE ML. \"\n",
        "                                \"Return ONLY valid JSON (no markdown).\"\n",
        "                            )\n",
        "                        },\n",
        "                        {\"role\": \"user\", \"content\": research_prompt}\n",
        "                    ],\n",
        "                    response_format={\"type\": \"json_object\"},  # <-- valid for Chat Completions\n",
        "                    temperature=0.0,  # deterministic; reduces junk\n",
        "                    top_p=1.0\n",
        "                    # (Set a request timeout at client level if your SDK supports it)\n",
        "                )\n",
        "\n",
        "                raw = completion.choices[0].message.content or \"{}\"\n",
        "                raw = _strip_markdown_fences(raw)\n",
        "\n",
        "                # Parse/validate\n",
        "                result = self._parse_and_filter(raw)\n",
        "\n",
        "                n = len(result.get(\"explainable_features\", []))\n",
        "                if n < self.require_min_features:\n",
        "                    raise ResearchFailedError(\n",
        "                        f\"Research returned {n} explainable feature(s), fewer than required minimum {self.require_min_features}.\"\n",
        "                    )\n",
        "\n",
        "                print(f\"  ‚úì Extracted {n} physiologically meaningful features\")\n",
        "                if result.get(\"insights_summary\"):\n",
        "                    summary = result[\"insights_summary\"][:100]\n",
        "                    print(f\"  üí° {summary}{'...' if len(result['insights_summary']) > 100 else ''}\")\n",
        "\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                last_exc = e\n",
        "                print(f\"  ‚ö† Research attempt {attempt+1}/{attempts} failed: {e}\")\n",
        "                traceback.print_exc()\n",
        "                if attempt < attempts - 1:\n",
        "                    time.sleep(self.backoff_sec)\n",
        "\n",
        "        # All attempts failed -> raise, so caller can abort without burning more tokens\n",
        "        raise ResearchFailedError(f\"Research failed after {attempts} attempt(s): {last_exc}\")\n",
        "\n",
        "    # ---------- helpers ----------\n",
        "\n",
        "    def _parse_and_filter(self, raw_json: str) -> Dict[str, Any]:\n",
        "        \"\"\"Parse JSON (optionally with Pydantic), enforce explainability filter, and required keys.\"\"\"\n",
        "        if _HAVE_PYDANTIC:\n",
        "            try:\n",
        "                parsed = PhysiologicalResearchResponse.model_validate_json(raw_json)\n",
        "                result = parsed.model_dump()\n",
        "            except ValidationError as ve:\n",
        "                # If schema fails, treat as failure (no fallback)\n",
        "                raise ResearchFailedError(f\"Invalid research JSON schema: {ve}\") from ve\n",
        "        else:\n",
        "            try:\n",
        "                result = json.loads(raw_json)\n",
        "                if not isinstance(result, dict):\n",
        "                    raise ResearchFailedError(\"Research response was not a JSON object.\")\n",
        "            except Exception as je:\n",
        "                raise ResearchFailedError(f\"Research JSON could not be parsed: {je}\") from je\n",
        "\n",
        "        # ensure keys\n",
        "        for k, default in [\n",
        "            (\"explainable_features\", []),\n",
        "            (\"domain_transformations\", []),\n",
        "            (\"explainable_interactions\", []),\n",
        "            (\"features_to_avoid\", []),\n",
        "            (\"insights_summary\", \"\")\n",
        "        ]:\n",
        "            result.setdefault(k, default)\n",
        "\n",
        "        # filter by explainability threshold\n",
        "        feats = result.get(\"explainable_features\", [])\n",
        "        filtered = []\n",
        "        for f in feats:\n",
        "            if not isinstance(f, dict):\n",
        "                continue\n",
        "            score = f.get(\"explainability_score\", 0) or 0\n",
        "            if score >= self.min_explainability:\n",
        "                filtered.append(f)\n",
        "        result[\"explainable_features\"] = filtered\n",
        "        return result\n",
        "\n",
        "    def _build_prompt(self, domain, problem, target, cols_disp, feedback) -> str:\n",
        "        lines = [\n",
        "            f\"Domain: {domain}\",\n",
        "            f\"Problem: {problem}\",\n",
        "            f\"Target variable: {target}\",\n",
        "            f\"Available columns: {cols_disp}\"\n",
        "        ]\n",
        "        if feedback:\n",
        "            lines.append(\"Previously successful features (if any):\")\n",
        "            lines.append(f\"  - {feedback.get('top_features', [])[:5]}\")\n",
        "            lines.append(f\"  - Best Score: {feedback.get('best_score', 'N/A')}\")\n",
        "\n",
        "        ctx = \"\\n\".join(lines)\n",
        "\n",
        "        # Keep the schema instruction tight to reduce hallucination + token use.\n",
        "        return f\"\"\"\n",
        "You are assisting clinical feature engineering for an interpretable stroke prediction model.\n",
        "\n",
        "{ctx}\n",
        "\n",
        "Return STRICT JSON with keys:\n",
        "- \"explainable_features\": list of objects with fields:\n",
        "  \"feature_concept\": string,\n",
        "  \"formula_pattern\": string (Pythonic expression using only available columns),\n",
        "  \"physiological_explanation\": string,\n",
        "  \"clinical_relevance\": string,\n",
        "  \"explainability_score\": integer in [1,5],\n",
        "  \"used_by\": string,\n",
        "  \"interpretation_guide\": {{\"high_values\": string, \"low_values\": string}},\n",
        "  \"source\": string\n",
        "- \"domain_transformations\": list of objects: {{\"name\": string, \"expr\": string}}\n",
        "- \"explainable_interactions\": list (each an array of 2 column names or an object)\n",
        "- \"features_to_avoid\": list of strings\n",
        "- \"insights_summary\": string\n",
        "\n",
        "Rules:\n",
        "- Focus on physiologically explainable features used in cardiovascular/neurology risk (stroke).\n",
        "- Prefer standard clinical indicators and clear mechanisms over black-box constructs.\n",
        "- Use only the provided columns in formula_pattern.\n",
        "- Output MUST be valid JSON. No markdown, no commentary outside JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "    def generate_explanation_report(self, research_results: Dict[str, Any], domain: str) -> str:\n",
        "        feats = research_results.get(\"explainable_features\", [])\n",
        "        if not feats:\n",
        "            return \"No explainable features returned.\"\n",
        "\n",
        "        lines = [\n",
        "            \"\\n\" + \"=\"*70,\n",
        "            \"PHYSIOLOGICALLY EXPLAINABLE FEATURES REPORT\",\n",
        "            f\"Domain: {domain}\",\n",
        "            \"=\"*70 + \"\\n\"\n",
        "        ]\n",
        "        for i, f in enumerate(feats, 1):\n",
        "            lines.append(f\"\\n{i}. {f.get('feature_concept','').upper()}\")\n",
        "            lines.append(\"   \" + \"‚îÄ\"*66)\n",
        "            lines.append(f\"   Formula: {f.get('formula_pattern','N/A')}\")\n",
        "            lines.append(\"\\n   Physiological Explanation:\")\n",
        "            lines.append(f\"   {f.get('physiological_explanation','N/A')}\")\n",
        "            lines.append(f\"\\n   Clinical Relevance: {f.get('clinical_relevance','UNKNOWN')}\")\n",
        "            lines.append(f\"   Explainability Score: {f.get('explainability_score', 0)}/5\")\n",
        "            lines.append(f\"   Used By: {f.get('used_by','Unknown')}\")\n",
        "            if 'interpretation_guide' in f and isinstance(f['interpretation_guide'], dict):\n",
        "                g = f['interpretation_guide']\n",
        "                lines.append(\"\\n   Interpretation:\")\n",
        "                lines.append(f\"   ‚Ä¢ High values ‚Üí {g.get('high_values','N/A')}\")\n",
        "                lines.append(f\"   ‚Ä¢ Low values ‚Üí {g.get('low_values','N/A')}\")\n",
        "            lines.append(f\"\\n   Source: {f.get('source','Not specified')}\\n\")\n",
        "\n",
        "        lines.append(\"\\n\" + \"=\"*70)\n",
        "        lines.append(\"SUMMARY\")\n",
        "        lines.append(\"=\"*70)\n",
        "        lines.append(research_results.get(\"insights_summary\", \"\"))\n",
        "        lines.append(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "        return \"\\n\".join(lines)\n"
      ],
      "metadata": {
        "id": "QfFGcC-fg8R0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureStrategyAgent:\n",
        "    \"\"\"Generates feature engineering strategies using LLM (THE DECISION MAKER)\"\"\"\n",
        "\n",
        "    def __init__(self, client, validator=None):\n",
        "        self.client = client\n",
        "        self.validator = validator\n",
        "\n",
        "    def design_strategy(self, X, target, feedback=None, research_context=None):\n",
        "        \"\"\"\n",
        "        Design a feature engineering strategy with KEEP/CREATE/REMOVE actions.\n",
        "        Now enforces physiological explainability requirements.\n",
        "        \"\"\"\n",
        "        prompt = self._build_strategy_prompt(X, target, feedback, research_context)\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"\"\"You are a feature engineering decision maker specializing in INTERPRETABLE features.\n",
        "\n",
        "CRITICAL: Every feature you create must be PHYSIOLOGICALLY/BIOLOGICALLY EXPLAINABLE.\n",
        "\n",
        "Priorities (in order):\n",
        "1. Clinical indicators and standard medical metrics\n",
        "2. Features with clear biological interpretation\n",
        "3. Explainable transformations that preserve meaning\n",
        "4. Research-backed feature interactions\n",
        "\n",
        "Always respond with valid JSON including physiological_rationale for each CREATE action.\"\"\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "            temperature=0.4\n",
        "        )\n",
        "\n",
        "        result = json.loads(response.choices[0].message.content)\n",
        "\n",
        "        print(f\"\\n  üìã Agent Strategy:\")\n",
        "        print(f\"    Domain: {result.get('domain', 'unknown')}\")\n",
        "        print(f\"    Reasoning: {result.get('reasoning', 'N/A')[:100]}...\")\n",
        "\n",
        "        # Count explainability scores\n",
        "        self._print_explainability_summary(result)\n",
        "\n",
        "        if self.validator:\n",
        "            validated_result = self.validator.validate_strategy(result, X, feedback)\n",
        "            return validated_result\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _build_strategy_prompt(self, X, target, feedback, research_context):\n",
        "        \"\"\"\n",
        "        Build the prompt for strategy generation with EXPLAINABILITY focus.\n",
        "        Enhanced to require physiological rationale for all features.\n",
        "        \"\"\"\n",
        "\n",
        "        # Feedback section\n",
        "        feedback_section = \"\"\n",
        "        if feedback:\n",
        "            feedback_section = f\"\"\"PREVIOUS ITERATION FEEDBACK:\n",
        "- Score: {feedback['best_score']:.3f}\n",
        "- Top 5 performing features: {feedback['top_features'][:5]}\n",
        "- What worked: {feedback.get('what_worked', 'N/A')}\n",
        "- What didn't work: {feedback.get('what_failed', 'N/A')}\n",
        "- Strategy: Build on what's working, avoid what failed\n",
        "\n",
        "üîç Explainability Analysis:\n",
        "{self._format_previous_explainability(feedback)}\n",
        "\"\"\"\n",
        "        else:\n",
        "            feedback_section = \"\"\"FIRST ITERATION: Explore PHYSIOLOGICALLY MEANINGFUL feature relationships\n",
        "\n",
        "Focus on:\n",
        "- Standard clinical indicators\n",
        "- Known biomarkers\n",
        "- Established medical calculations\n",
        "- Explainable interactions\"\"\"\n",
        "\n",
        "        # Research section - ENHANCED for physiological features\n",
        "        research_section = \"\"\n",
        "        if research_context:\n",
        "            research_section = self._format_physiological_research(research_context)\n",
        "\n",
        "        # Get column info with types\n",
        "        column_info = X.columns.tolist()\n",
        "        if hasattr(X, 'dtypes'):\n",
        "            column_info = [f\"{col} ({dtype})\" for col, dtype in zip(X.columns, X.dtypes)]\n",
        "\n",
        "        # BUILD THE COMPLETE PROMPT WITH EXPLAINABILITY REQUIREMENTS\n",
        "        return f\"\"\"You are the DECISION MAKER for PHYSIOLOGICALLY EXPLAINABLE feature engineering.\n",
        "\n",
        "DATASET INFO:\n",
        "- Target: {target}\n",
        "- Current Columns ({len(X.columns)}): {column_info}\n",
        "- Shape: {X.shape}\n",
        "- Available columns list: {X.columns.tolist()}\n",
        "\n",
        "{feedback_section}\n",
        "\n",
        "{research_section}\n",
        "\n",
        "YOUR TASK: Design a feature engineering strategy prioritizing EXPLAINABILITY.\n",
        "\n",
        "üéØ EXPLAINABILITY REQUIREMENTS:\n",
        "\n",
        "Every CREATE action MUST include:\n",
        "1. **physiological_rationale** - WHY this feature is biologically meaningful\n",
        "2. **explainability_score** (1-5):\n",
        "   - 5 = Standard clinical metric (e.g., BMI, eGFR, cholesterol/HDL)\n",
        "   - 4 = Clear biological meaning (e.g., age √ó kidney_function - aging affects kidneys)\n",
        "   - 3 = Interpretable transformation (e.g., log(glucose) - normalizes skewed data)\n",
        "   - 2 = Weak biological basis (e.g., age^2 - captures curve but unclear WHY)\n",
        "   - 1 = \"Black box\" (e.g., complex polynomial - AVOID)\n",
        "3. **clinical_relevance** - HIGH/MEDIUM/LOW\n",
        "4. **interpretation** - What does high/low value mean?\n",
        "\n",
        "ACTIONS YOU MUST TAKE:\n",
        "\n",
        "1. **KEEP** - Preserve existing features that are valuable\n",
        "   - MANDATORY: Keep all top-performing features from previous iteration\n",
        "   - Keep features that align with research insights\n",
        "   - Keep features with high explainability scores\n",
        "\n",
        "2. **CREATE** - Generate new EXPLAINABLE features\n",
        "\n",
        "   ‚úÖ HIGH PRIORITY (Explainability 4-5):\n",
        "   - Clinical ratios (e.g., cholesterol/HDL - standard medical metric)\n",
        "   - Standard biomarkers (e.g., BMI calculation if height/weight available)\n",
        "   - Known medical formulas (e.g., eGFR, ASCVD risk score components)\n",
        "   - Physiological interactions (e.g., age √ó kidney_function - biological aging)\n",
        "\n",
        "   ‚ö†Ô∏è MEDIUM PRIORITY (Explainability 3):\n",
        "   - Interpretable transformations (log for skewed data, sqrt for variance)\n",
        "   - Clinical categories/binning (e.g., blood pressure stages)\n",
        "   - Simple polynomials IF explainable (age^2 for accelerating risk)\n",
        "\n",
        "   ‚ùå AVOID (Explainability 1-2):\n",
        "   - Complex polynomials without biological meaning\n",
        "   - Arbitrary mathematical transformations\n",
        "   - Features that can't be explained to a clinician\n",
        "\n",
        "   CRITICAL RULES:\n",
        "   ‚úì Use ONLY EXISTING column names from: {X.columns.tolist()}\n",
        "   ‚úì DO NOT reference features being created in this same iteration\n",
        "   ‚úì Each CREATE formula can ONLY use columns that currently exist\n",
        "   ‚úì If previous iteration had good features, KEEP them first\n",
        "   ‚úì Formulas must be executable Python (use np.log, np.sqrt, np.abs, etc.)\n",
        "   ‚úì Aim for 8-12 total actions (balance keep + create, quality > quantity)\n",
        "\n",
        "3. **REMOVE** - Drop features that aren't helping (OPTIONAL)\n",
        "   - Remove low explainability features that don't perform well\n",
        "   - Remove redundant features\n",
        "\n",
        "OUTPUT FORMAT (JSON only):\n",
        "{{\n",
        "  \"domain\": \"identified domain (e.g., healthcare - cardiology)\",\n",
        "  \"reasoning\": \"Your strategy explanation with focus on explainability\",\n",
        "  \"explainability_focus\": \"Why these features are physiologically meaningful\",\n",
        "  \"actions\": [\n",
        "    {{\n",
        "      \"action\": \"keep\",\n",
        "      \"name\": \"bmi\",\n",
        "      \"why\": \"Top performing feature from previous iteration\",\n",
        "      \"explainability_score\": 5,\n",
        "      \"physiological_rationale\": \"Body Mass Index - standard clinical obesity metric\"\n",
        "    }},\n",
        "    {{\n",
        "      \"action\": \"create\",\n",
        "      \"name\": \"insulin_glucose_ratio\",\n",
        "      \"formula\": \"df['insulin'] / (df['glucose'] + 1)\",\n",
        "      \"why\": \"Standard endocrinology metric - measures pancreatic function\",\n",
        "      \"physiological_rationale\": \"Ratio of insulin to glucose indicates pancreatic beta cell function. Low ratio suggests insulin resistance, a key Type 2 diabetes mechanism. Used by endocrinologists for diagnosis.\",\n",
        "      \"explainability_score\": 5,\n",
        "      \"clinical_relevance\": \"HIGH\",\n",
        "      \"interpretation\": {{\n",
        "        \"high\": \"Good insulin production (healthy)\",\n",
        "        \"low\": \"Insulin resistance or pancreatic dysfunction (diabetes risk)\"\n",
        "      }},\n",
        "      \"source\": \"American Diabetes Association guidelines\"\n",
        "    }},\n",
        "    {{\n",
        "      \"action\": \"create\",\n",
        "      \"name\": \"age_kidney_interaction\",\n",
        "      \"formula\": \"df['age'] * df['kidney_function_score']\",\n",
        "      \"why\": \"Aging compounds kidney disease effects\",\n",
        "      \"physiological_rationale\": \"Kidney function naturally declines with age (~1 ml/min/year after age 30). This interaction captures compounded risk from both aging and reduced kidney function.\",\n",
        "      \"explainability_score\": 4,\n",
        "      \"clinical_relevance\": \"MEDIUM\",\n",
        "      \"interpretation\": {{\n",
        "        \"high\": \"Older patient with kidney issues - compound risk\",\n",
        "        \"low\": \"Young or good kidney function - lower risk\"\n",
        "      }},\n",
        "      \"source\": \"Nephrology literature on age-related GFR decline\"\n",
        "    }},\n",
        "    {{\n",
        "      \"action\": \"create\",\n",
        "      \"name\": \"glucose_log\",\n",
        "      \"formula\": \"np.log1p(df['glucose'])\",\n",
        "      \"why\": \"Normalizes right-skewed glucose distribution\",\n",
        "      \"physiological_rationale\": \"Glucose values are right-skewed. Log transform normalizes distribution while preserving relative differences. Common preprocessing for skewed medical measurements.\",\n",
        "      \"explainability_score\": 3,\n",
        "      \"clinical_relevance\": \"MEDIUM\",\n",
        "      \"interpretation\": {{\n",
        "        \"high\": \"High glucose (preserves clinical meaning)\",\n",
        "        \"low\": \"Normal glucose (compressed very low values)\"\n",
        "      }},\n",
        "      \"source\": \"Standard statistical preprocessing for biomedical data\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "CRITICAL:\n",
        "- Target 8-12 actions with explainability_score >= 3\n",
        "- Every CREATE action MUST have physiological_rationale\n",
        "- Prioritize features clinicians can understand and trust\n",
        "- ONLY use existing columns: {X.columns.tolist()}\"\"\"\n",
        "\n",
        "    def _format_physiological_research(self, research_context):\n",
        "        \"\"\"Format research context with emphasis on explainability\"\"\"\n",
        "\n",
        "        # Handle both old structure and new physiological structure\n",
        "        if 'explainable_features' in research_context:\n",
        "            # New physiological structure\n",
        "            section = \"\"\"RESEARCH-BACKED PHYSIOLOGICALLY EXPLAINABLE INSIGHTS:\n",
        "\n",
        "üî¨ Explainable Features Found:\n",
        "\"\"\"\n",
        "            for feature in research_context.get('explainable_features', [])[:4]:\n",
        "                section += f\"\"\"\n",
        "  ‚Ä¢ {feature.get('feature_concept', 'Unknown')} (Score: {feature.get('explainability_score', 'N/A')}/5)\n",
        "    Formula: {feature.get('formula_pattern', 'N/A')}\n",
        "    Why: {feature.get('physiological_explanation', 'N/A')[:120]}...\n",
        "    Clinical Relevance: {feature.get('clinical_relevance', 'UNKNOWN')}\n",
        "\"\"\"\n",
        "\n",
        "            if research_context.get('domain_transformations'):\n",
        "                section += \"\\n‚öôÔ∏è Domain-Specific Transformations:\\n\"\n",
        "                for trans in research_context.get('domain_transformations', [])[:3]:\n",
        "                    section += f\"  ‚Ä¢ {trans.get('transformation', trans) if isinstance(trans, dict) else trans}\\n\"\n",
        "\n",
        "            if research_context.get('explainable_interactions'):\n",
        "                section += \"\\nüîó Explainable Interactions:\\n\"\n",
        "                for inter in research_context.get('explainable_interactions', [])[:3]:\n",
        "                    section += f\"  ‚Ä¢ {inter.get('interaction', inter) if isinstance(inter, dict) else inter}\\n\"\n",
        "\n",
        "            if research_context.get('features_to_avoid'):\n",
        "                section += \"\\n‚ö†Ô∏è Features to AVOID (Low Explainability):\\n\"\n",
        "                for avoid in research_context.get('features_to_avoid', [])[:3]:\n",
        "                    section += f\"  ‚Ä¢ {avoid.get('feature', avoid) if isinstance(avoid, dict) else avoid}\\n\"\n",
        "\n",
        "        else:\n",
        "            # Old structure - still usable\n",
        "            section = \"\"\"RESEARCH-BACKED DOMAIN INSIGHTS:\n",
        "\n",
        "üìä Domain Patterns Found:\n",
        "\"\"\"\n",
        "            section += \"\\n\".join(f\"  ‚Ä¢ {p}\" for p in research_context.get('domain_patterns', [])[:4])\n",
        "\n",
        "            section += \"\\n\\nüîß Recommended Feature Types:\\n\"\n",
        "            section += \"\\n\".join(f\"  ‚Ä¢ {t}\" for t in research_context.get('recommended_feature_types', [])[:4])\n",
        "\n",
        "            if research_context.get('domain_transformations'):\n",
        "                section += \"\\n\\n‚öôÔ∏è Domain-Specific Transformations:\\n\"\n",
        "                section += \"\\n\".join(f\"  ‚Ä¢ {t}\" for t in research_context.get('domain_transformations', [])[:3])\n",
        "\n",
        "            if research_context.get('key_interactions'):\n",
        "                section += \"\\n\\nüîó Key Interactions to Consider:\\n\"\n",
        "                section += \"\\n\".join(f\"  ‚Ä¢ {i}\" for i in research_context.get('key_interactions', [])[:3])\n",
        "\n",
        "            if research_context.get('pitfalls_to_avoid'):\n",
        "                section += \"\\n\\n‚ö†Ô∏è Pitfalls to Avoid:\\n\"\n",
        "                section += \"\\n\".join(f\"  ‚Ä¢ {p}\" for p in research_context.get('pitfalls_to_avoid', [])[:3])\n",
        "\n",
        "        section += f\"\\n\\nüí° Research Summary: {research_context.get('insights_summary', '')}\\n\"\n",
        "        section += \"\\nUSE these insights to create EXPLAINABLE features.\\n\"\n",
        "\n",
        "        return section\n",
        "\n",
        "    def _format_previous_explainability(self, feedback):\n",
        "        \"\"\"Format explainability information from previous iteration\"\"\"\n",
        "\n",
        "        if 'feature_explainability' in feedback:\n",
        "            lines = []\n",
        "            for feature, info in list(feedback['feature_explainability'].items())[:5]:\n",
        "                score = info.get('score', 'N/A')\n",
        "                lines.append(f\"  - {feature}: Score {score}/5\")\n",
        "            return \"\\n\".join(lines) if lines else \"  No explainability data from previous iteration\"\n",
        "\n",
        "        return \"  Explainability tracking not available from previous iteration\"\n",
        "\n",
        "    def _print_explainability_summary(self, result):\n",
        "        \"\"\"Print summary of explainability scores for created features\"\"\"\n",
        "\n",
        "        create_actions = [a for a in result.get('actions', []) if a.get('action') == 'create']\n",
        "\n",
        "        if not create_actions:\n",
        "            return\n",
        "\n",
        "        scores = [a.get('explainability_score', 0) for a in create_actions]\n",
        "\n",
        "        if scores:\n",
        "            avg_score = sum(scores) / len(scores)\n",
        "            high_quality = sum(1 for s in scores if s >= 4)\n",
        "            medium_quality = sum(1 for s in scores if 3 <= s < 4)\n",
        "            low_quality = sum(1 for s in scores if s < 3)\n",
        "\n",
        "            print(f\"    Explainability: Avg {avg_score:.1f}/5\")\n",
        "            print(f\"    Quality: {high_quality} high (4-5), {medium_quality} medium (3), {low_quality} low (<3)\")\n",
        "\n",
        "    def execute_actions(self, df, actions, verbose=True):\n",
        "        \"\"\"\n",
        "        Execute validated actions on the dataframe.\n",
        "        Enhanced with explainability tracking.\n",
        "\n",
        "        ASSUMES: Actions have already been validated by FeatureValidator\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'kept': [],\n",
        "            'created': [],\n",
        "            'removed': [],\n",
        "            'failed': [],\n",
        "            'explainability_scores': {}  # NEW: Track explainability\n",
        "        }\n",
        "\n",
        "        print(\"\\n  üîß Executing actions...\")\n",
        "\n",
        "        for action in actions:\n",
        "            action_type = action.get('action')\n",
        "            name = action.get('name')\n",
        "\n",
        "            try:\n",
        "                if action_type == 'keep':\n",
        "                    results['kept'].append(name)\n",
        "                    if verbose:\n",
        "                        exp_score = action.get('explainability_score', 'N/A')\n",
        "                        print(f\"  ‚úì Kept: {name} (Explainability: {exp_score}/5)\")\n",
        "\n",
        "                    # Track explainability\n",
        "                    if 'explainability_score' in action:\n",
        "                        results['explainability_scores'][name] = {\n",
        "                            'score': action['explainability_score'],\n",
        "                            'rationale': action.get('physiological_rationale', 'N/A')\n",
        "                        }\n",
        "\n",
        "                elif action_type == 'create':\n",
        "                    formula = action.get('formula')\n",
        "\n",
        "                    # Execute the formula\n",
        "                    df[name] = eval(formula)\n",
        "                    results['created'].append(name)\n",
        "\n",
        "                    if verbose:\n",
        "                        exp_score = action.get('explainability_score', '?')\n",
        "                        clinical = action.get('clinical_relevance', '?')\n",
        "                        print(f\"  ‚úì Created: {name} (Explainability: {exp_score}/5, Clinical: {clinical})\")\n",
        "\n",
        "                    # Track explainability\n",
        "                    results['explainability_scores'][name] = {\n",
        "                        'score': action.get('explainability_score', 0),\n",
        "                        'rationale': action.get('physiological_rationale', 'Not provided'),\n",
        "                        'clinical_relevance': action.get('clinical_relevance', 'UNKNOWN'),\n",
        "                        'interpretation': action.get('interpretation', {})\n",
        "                    }\n",
        "\n",
        "                elif action_type == 'remove':\n",
        "                    df.drop(columns=[name], inplace=True)\n",
        "                    results['removed'].append(name)\n",
        "                    if verbose:\n",
        "                        print(f\"  ‚úì Removed: {name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"  ‚ùå Failed {action_type} for '{name}': {e}\")\n",
        "                results['failed'].append({\n",
        "                    'action': action,\n",
        "                    'error': str(e)\n",
        "                })\n",
        "\n",
        "        # Print explainability summary\n",
        "        if results['explainability_scores']:\n",
        "            avg_score = sum(s['score'] for s in results['explainability_scores'].values()) / len(results['explainability_scores'])\n",
        "            high_quality = sum(1 for s in results['explainability_scores'].values() if s['score'] >= 4)\n",
        "\n",
        "            print(f\"\\n  üìä Execution complete: {len(results['created'])} created, \"\n",
        "                  f\"{len(results['kept'])} kept, {len(results['removed'])} removed, \"\n",
        "                  f\"{len(results['failed'])} failed\")\n",
        "            print(f\"  üîç Explainability: Avg {avg_score:.1f}/5, {high_quality} highly explainable (‚â•4)\")\n",
        "        else:\n",
        "            print(f\"\\n  üìä Execution complete: {len(results['created'])} created, \"\n",
        "                  f\"{len(results['kept'])} kept, {len(results['removed'])} removed, \"\n",
        "                  f\"{len(results['failed'])} failed\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def generate_feature_documentation(self, actions, domain):\n",
        "        \"\"\"\n",
        "        Generate human-readable documentation for all features.\n",
        "        Explains what each feature means physiologically.\n",
        "        \"\"\"\n",
        "\n",
        "        doc_lines = [\n",
        "            f\"\\n{'='*70}\",\n",
        "            f\"FEATURE DOCUMENTATION - {domain.upper()}\",\n",
        "            f\"{'='*70}\\n\"\n",
        "        ]\n",
        "\n",
        "        create_actions = [a for a in actions if a.get('action') == 'create']\n",
        "        keep_actions = [a for a in actions if a.get('action') == 'keep']\n",
        "\n",
        "        if create_actions:\n",
        "            doc_lines.append(\"NEWLY CREATED FEATURES:\")\n",
        "            doc_lines.append(\"‚îÄ\" * 70)\n",
        "\n",
        "            for idx, action in enumerate(create_actions, 1):\n",
        "                doc_lines.append(f\"\\n{idx}. {action['name']}\")\n",
        "                doc_lines.append(f\"   Formula: {action.get('formula', 'N/A')}\")\n",
        "                doc_lines.append(f\"   Explainability: {action.get('explainability_score', '?')}/5\")\n",
        "                doc_lines.append(f\"   Clinical Relevance: {action.get('clinical_relevance', 'UNKNOWN')}\")\n",
        "                doc_lines.append(f\"\\n   Physiological Meaning:\")\n",
        "                doc_lines.append(f\"   {action.get('physiological_rationale', 'Not provided')}\")\n",
        "\n",
        "                if 'interpretation' in action and action['interpretation']:\n",
        "                    interp = action['interpretation']\n",
        "                    doc_lines.append(f\"\\n   Interpretation:\")\n",
        "                    doc_lines.append(f\"   ‚Ä¢ High values: {interp.get('high', 'N/A')}\")\n",
        "                    doc_lines.append(f\"   ‚Ä¢ Low values: {interp.get('low', 'N/A')}\")\n",
        "\n",
        "                if 'source' in action:\n",
        "                    doc_lines.append(f\"\\n   Source: {action['source']}\")\n",
        "\n",
        "                doc_lines.append(\"\")\n",
        "\n",
        "        if keep_actions:\n",
        "            doc_lines.append(f\"\\n{'‚îÄ' * 70}\")\n",
        "            doc_lines.append(\"KEPT FEATURES (from previous iteration):\")\n",
        "            doc_lines.append(\"‚îÄ\" * 70)\n",
        "\n",
        "            for action in keep_actions:\n",
        "                exp_score = action.get('explainability_score', 'N/A')\n",
        "                doc_lines.append(f\"  ‚Ä¢ {action['name']} (Explainability: {exp_score}/5)\")\n",
        "                if 'physiological_rationale' in action:\n",
        "                    doc_lines.append(f\"    {action['physiological_rationale'][:80]}...\")\n",
        "\n",
        "        doc_lines.append(f\"\\n{'='*70}\\n\")\n",
        "\n",
        "        return \"\\n\".join(doc_lines)"
      ],
      "metadata": {
        "id": "J0e6GARzh3M9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureValidator:\n",
        "    \"\"\"Validates feature engineering actions and formulas with comprehensive checks\"\"\"\n",
        "\n",
        "    def __init__(self, client, min_explainability_score=3):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            client: OpenAI client for LLM-based self-correction\n",
        "            min_explainability_score: Minimum explainability score (1-5) to accept features\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.min_explainability_score = min_explainability_score\n",
        "\n",
        "    def validate_strategy(self, result, X, feedback=None, max_retry_attempts=2):\n",
        "        \"\"\"\n",
        "        Validate and sanitize agent suggestions with explainability checks.\n",
        "        If validation fails, pass errors back to LLM for self-correction.\n",
        "\n",
        "        Args:\n",
        "            result: Strategy result from FeatureStrategyAgent\n",
        "            X: DataFrame with current features\n",
        "            feedback: Feedback from previous iteration\n",
        "            max_retry_attempts: How many times to let LLM retry fixing errors\n",
        "\n",
        "        Returns:\n",
        "            Validated result with valid actions\n",
        "        \"\"\"\n",
        "        print(\"  üîç Validating strategy (technical + explainability)...\")\n",
        "\n",
        "        # Try validation with potential retries\n",
        "        for attempt in range(max_retry_attempts + 1):\n",
        "            valid_actions = []\n",
        "            validation_errors = []\n",
        "            existing_cols = set(X.columns)\n",
        "\n",
        "            # Step 1: Auto-keep top features (even if LLM forgot)\n",
        "            auto_keeps = self._auto_keep_top_features(result, X, feedback)\n",
        "            valid_actions.extend(auto_keeps)\n",
        "\n",
        "            # Step 2: Validate each action from LLM\n",
        "            for action in result.get('actions', []):\n",
        "                validated, error = self._validate_action_with_error(\n",
        "                    action, existing_cols, feedback, valid_actions, X\n",
        "                )\n",
        "                if validated:\n",
        "                    valid_actions.append(validated)\n",
        "                else:\n",
        "                    validation_errors.append({\n",
        "                        'action': action,\n",
        "                        'error': error\n",
        "                    })\n",
        "\n",
        "            # Step 3: Filter by explainability\n",
        "            if self.min_explainability_score > 0:\n",
        "                valid_actions, low_explainability_errors = self._filter_by_explainability(\n",
        "                    valid_actions,\n",
        "                    X,\n",
        "                    min_score=self.min_explainability_score\n",
        "                )\n",
        "                validation_errors.extend(low_explainability_errors)\n",
        "\n",
        "            # Step 4: Check if we have enough valid actions\n",
        "            if len(valid_actions) >= 3:  # Success threshold\n",
        "                result['actions'] = valid_actions\n",
        "                result['validation_errors'] = validation_errors\n",
        "                self._print_validation_summary(valid_actions, validation_errors)\n",
        "                return result\n",
        "\n",
        "            # Step 5: If failed and have retries left, ask LLM to fix\n",
        "            if attempt < max_retry_attempts and validation_errors:\n",
        "                print(f\"\\n  ‚ö† Validation issues found. Attempt {attempt + 1}/{max_retry_attempts} - asking LLM to fix...\")\n",
        "                result = self._request_llm_correction(result, validation_errors, X, feedback)\n",
        "                # Loop will retry with corrected result\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # Step 6: Fallback if everything failed\n",
        "        if len(valid_actions) < 3:\n",
        "            print(\"  ‚ö† WARNING: Insufficient valid actions after retries! Using fallback...\")\n",
        "            valid_actions.extend(self._create_fallback_actions(X, feedback))\n",
        "\n",
        "        result['actions'] = valid_actions\n",
        "        result['validation_errors'] = validation_errors\n",
        "        self._print_validation_summary(valid_actions, validation_errors)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _validate_action_with_error(self, action, existing_cols, feedback, valid_actions, X):\n",
        "        \"\"\"\n",
        "        Validate a single action and return both result and error message.\n",
        "\n",
        "        Returns:\n",
        "            (validated_action, error_message)\n",
        "            - If valid: (action_dict, None)\n",
        "            - If invalid: (None, error_string)\n",
        "        \"\"\"\n",
        "        action_type = action.get('action', '').lower()\n",
        "        name = action.get('name', '')\n",
        "\n",
        "        if not name:\n",
        "            return None, \"Action has no name\"\n",
        "\n",
        "        if action_type == 'keep':\n",
        "            return self._validate_keep_with_error(action, name, existing_cols, valid_actions)\n",
        "\n",
        "        elif action_type == 'create':\n",
        "            return self._validate_create_with_error(action, name, X, existing_cols, valid_actions)\n",
        "\n",
        "        elif action_type == 'remove':\n",
        "            return self._validate_remove_with_error(action, name, existing_cols, feedback)\n",
        "\n",
        "        else:\n",
        "            return None, f\"Unknown action type: '{action_type}'\"\n",
        "\n",
        "    def _validate_keep_with_error(self, action, name, existing_cols, valid_actions):\n",
        "        \"\"\"Validate keep action, return error if invalid\"\"\"\n",
        "        if name not in existing_cols:\n",
        "            return None, f\"Cannot keep '{name}' - not in dataset (available: {list(existing_cols)[:10]})\"\n",
        "\n",
        "        # Check for duplicate keeps\n",
        "        if any(a['action'] == 'keep' and a['name'] == name for a in valid_actions):\n",
        "            return None, f\"Duplicate keep for '{name}'\"\n",
        "\n",
        "        exp_score = action.get('explainability_score', 'N/A')\n",
        "        print(f\"  ‚úì Keep: {name} (Explainability: {exp_score})\")\n",
        "        return action, None\n",
        "\n",
        "    def _validate_create_with_error(self, action, name, X, existing_cols, valid_actions=None):\n",
        "        \"\"\"Validate create action with comprehensive error reporting\"\"\"\n",
        "        formula = action.get('formula', '')\n",
        "\n",
        "        if not formula:\n",
        "            return None, f\"Cannot create '{name}' - no formula provided\"\n",
        "\n",
        "        # Check for explainability metadata first (so LLM knows to add it)\n",
        "        exp_error = self._check_explainability_metadata(action, name)\n",
        "        if exp_error:\n",
        "            return None, exp_error\n",
        "\n",
        "        available_set = set(existing_cols)\n",
        "        if valid_actions:\n",
        "            for a in valid_actions:\n",
        "                try:\n",
        "                    if a.get('action','').lower() == 'create' and a.get('name'):\n",
        "                        available_set.add(a['name'])\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        # Technical validation (formula correctness)\n",
        "        validation_result = self.validate_formula(\n",
        "        formula, X, name,\n",
        "        available_columns_override=available_set\n",
        "          )\n",
        "\n",
        "        if not validation_result['valid']:\n",
        "            error_msg = f\"Invalid formula for '{name}': {validation_result['error']}\"\n",
        "            if validation_result.get('details'):\n",
        "                error_msg += f\". Details: {validation_result['details']}\"\n",
        "            return None, error_msg\n",
        "\n",
        "        exp_score = action.get('explainability_score', '?')\n",
        "        clinical = action.get('clinical_relevance', '?')\n",
        "        print(f\"  ‚úì Create: {name} (Explainability: {exp_score}/5, Clinical: {clinical})\")\n",
        "\n",
        "        if validation_result.get('warning'):\n",
        "            print(f\"    ‚ö† Warning: {validation_result['warning']}\")\n",
        "\n",
        "        return action, None\n",
        "\n",
        "    def _validate_remove_with_error(self, action, name, existing_cols, feedback):\n",
        "        \"\"\"Validate remove action with error reporting\"\"\"\n",
        "        if name not in existing_cols:\n",
        "            return None, f\"Cannot remove '{name}' - not in dataset\"\n",
        "\n",
        "        # Block removal of top features\n",
        "        if feedback and name in feedback.get('top_features', [])[:5]:\n",
        "            return None, f\"Cannot remove '{name}' - it's a top-5 performing feature\"\n",
        "\n",
        "        print(f\"  ‚úì Remove: {name}\")\n",
        "        return action, None\n",
        "\n",
        "    def _check_explainability_metadata(self, action, name):\n",
        "        \"\"\"\n",
        "        Check if action has required explainability metadata.\n",
        "        Returns error string if invalid, None if valid.\n",
        "        \"\"\"\n",
        "        required_fields = ['physiological_rationale', 'explainability_score']\n",
        "        missing_fields = [f for f in required_fields if f not in action]\n",
        "\n",
        "        if missing_fields:\n",
        "            return f\"Missing required explainability fields for '{name}': {missing_fields}. Every CREATE action must include physiological_rationale and explainability_score (1-5).\"\n",
        "\n",
        "        # Validate explainability_score is in valid range\n",
        "        score = action.get('explainability_score')\n",
        "        if not isinstance(score, (int, float)) or score < 1 or score > 5:\n",
        "            return f\"Invalid explainability_score for '{name}': {score}. Must be integer 1-5.\"\n",
        "\n",
        "        return None  # Valid\n",
        "\n",
        "    def _filter_by_explainability(self, actions, X, min_score=3):\n",
        "        \"\"\"\n",
        "        Filter actions based on explainability score.\n",
        "\n",
        "        Returns:\n",
        "            (valid_actions, error_list)\n",
        "        \"\"\"\n",
        "        valid = []\n",
        "        errors = []\n",
        "\n",
        "        for action in actions:\n",
        "            # KEEP and REMOVE actions always pass\n",
        "            if action['action'] in ['keep', 'remove']:\n",
        "                valid.append(action)\n",
        "                continue\n",
        "\n",
        "            # For CREATE actions, check explainability score\n",
        "            exp_score = action.get('explainability_score')\n",
        "\n",
        "            # If no score provided (shouldn't happen after metadata check, but just in case)\n",
        "            if exp_score is None:\n",
        "                errors.append({\n",
        "                    'action': action,\n",
        "                    'error': f\"No explainability_score for '{action['name']}'\"\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            if exp_score >= min_score:\n",
        "                valid.append(action)\n",
        "            else:\n",
        "                error_msg = (f\"Feature '{action['name']}' has explainability_score {exp_score}, \"\n",
        "                           f\"which is below minimum threshold of {min_score}. \"\n",
        "                           f\"Feature rationale: '{action.get('physiological_rationale', 'N/A')}'. \"\n",
        "                           f\"Either improve the explainability or increase the score if the rationale is actually sound.\")\n",
        "                print(f\"    ‚ùå Rejecting '{action['name']}' - explainability too low ({exp_score}/{min_score})\")\n",
        "                errors.append({\n",
        "                    'action': action,\n",
        "                    'error': error_msg\n",
        "                })\n",
        "\n",
        "        return valid, errors\n",
        "\n",
        "    def _request_llm_correction(self, result, validation_errors, X, feedback):\n",
        "        \"\"\"\n",
        "        Pass validation errors back to LLM and request corrected strategy.\n",
        "\n",
        "        This is the key feature: LLM sees what went wrong and fixes it.\n",
        "        \"\"\"\n",
        "        # Build error summary\n",
        "        error_summary = self._format_errors_for_llm(validation_errors, X)\n",
        "\n",
        "        correction_prompt = f\"\"\"Your previous feature engineering strategy had validation errors. Please fix them.\n",
        "\n",
        "ORIGINAL STRATEGY:\n",
        "Domain: {result.get('domain', 'unknown')}\n",
        "Reasoning: {result.get('reasoning', 'N/A')}\n",
        "\n",
        "VALIDATION ERRORS ({len(validation_errors)} issues):\n",
        "{error_summary}\n",
        "\n",
        "CURRENT DATASET COLUMNS:\n",
        "{X.columns.tolist()}\n",
        "\n",
        "YOUR TASK: Generate a CORRECTED strategy that fixes all validation errors.\n",
        "\n",
        "Common fixes needed:\n",
        "1. Missing explainability metadata? ‚Üí Add physiological_rationale and explainability_score (1-5)\n",
        "2. Referencing non-existent columns? ‚Üí Use only columns from the current dataset list above\n",
        "3. Low explainability score? ‚Üí Either improve the rationale or create a more explainable feature\n",
        "4. Invalid formula? ‚Üí Fix syntax, avoid dangerous operations, ensure columns exist\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "‚úì Every CREATE action must have: physiological_rationale, explainability_score (1-5), clinical_relevance\n",
        "‚úì Only reference columns that exist in dataset: {X.columns.tolist()}\n",
        "‚úì Explainability scores must be >= {self.min_explainability_score} (prioritize clinically meaningful features)\n",
        "‚úì Formulas must be valid Python/pandas\n",
        "\n",
        "OUTPUT FORMAT (JSON only):\n",
        "{{\n",
        "  \"domain\": \"domain\",\n",
        "  \"reasoning\": \"Why this corrected strategy will work\",\n",
        "  \"actions\": [\n",
        "    {{\n",
        "      \"action\": \"create\",\n",
        "      \"name\": \"feature_name\",\n",
        "      \"formula\": \"df['col1'] * df['col2']\",\n",
        "      \"why\": \"Brief explanation\",\n",
        "      \"physiological_rationale\": \"Detailed biological/medical explanation of why this feature is meaningful\",\n",
        "      \"explainability_score\": 4,\n",
        "      \"clinical_relevance\": \"HIGH/MEDIUM/LOW\",\n",
        "      \"interpretation\": {{\n",
        "        \"high\": \"What high values mean\",\n",
        "        \"low\": \"What low values mean\"\n",
        "      }}\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Generate 8-12 actions. Fix all errors listed above.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are a feature engineering expert fixing validation errors. Always respond with valid JSON.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": correction_prompt\n",
        "                    }\n",
        "                ],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.4,\n",
        "                max_tokens=2000\n",
        "            )\n",
        "\n",
        "            corrected_result = json.loads(response.choices[0].message.content)\n",
        "            print(f\"  ‚úì LLM generated corrected strategy with {len(corrected_result.get('actions', []))} actions\")\n",
        "\n",
        "            return corrected_result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† LLM correction failed: {e}\")\n",
        "            return result  # Return original if correction fails\n",
        "\n",
        "    def _format_errors_for_llm(self, validation_errors, X):\n",
        "        \"\"\"Format validation errors in a clear way for LLM to understand and fix\"\"\"\n",
        "        error_lines = []\n",
        "\n",
        "        for idx, error_item in enumerate(validation_errors, 1):\n",
        "            action = error_item['action']\n",
        "            error = error_item['error']\n",
        "\n",
        "            error_lines.append(f\"\\n{idx}. Feature: {action.get('name', 'unknown')}\")\n",
        "            error_lines.append(f\"   Action: {action.get('action', 'unknown')}\")\n",
        "            error_lines.append(f\"   ERROR: {error}\")\n",
        "\n",
        "            if action.get('formula'):\n",
        "                error_lines.append(f\"   Formula: {action['formula']}\")\n",
        "            if action.get('physiological_rationale'):\n",
        "                error_lines.append(f\"   Rationale: {action['physiological_rationale'][:100]}...\")\n",
        "\n",
        "        return \"\\n\".join(error_lines)\n",
        "\n",
        "    def _auto_keep_top_features(self, result, X, feedback):\n",
        "        \"\"\"\n",
        "        Automatically keep top-performing features from previous iteration.\n",
        "        Includes explainability metadata if available.\n",
        "        \"\"\"\n",
        "        if not feedback or not feedback.get('top_features'):\n",
        "            return []\n",
        "\n",
        "        top_features = feedback['top_features'][:3]\n",
        "        suggested_keeps = {\n",
        "            a['name'] for a in result.get('actions', [])\n",
        "            if a.get('action') == 'keep'\n",
        "        }\n",
        "        existing_cols = set(X.columns)\n",
        "\n",
        "        auto_keeps = []\n",
        "        for feature in top_features:\n",
        "            if feature in existing_cols and feature not in suggested_keeps:\n",
        "                keep_action = {\n",
        "                    \"action\": \"keep\",\n",
        "                    \"name\": feature,\n",
        "                    \"why\": f\"Auto-kept: top feature from previous iteration (score: {feedback.get('best_score', 'N/A')})\"\n",
        "                }\n",
        "\n",
        "                # Preserve explainability metadata if available\n",
        "                if 'feature_explainability' in feedback and feature in feedback['feature_explainability']:\n",
        "                    exp_data = feedback['feature_explainability'][feature]\n",
        "                    keep_action['explainability_score'] = exp_data.get('score', None)\n",
        "                    keep_action['physiological_rationale'] = exp_data.get('rationale', None)\n",
        "\n",
        "                auto_keeps.append(keep_action)\n",
        "                print(f\"  ‚ö† Auto-keeping top feature: {feature}\")\n",
        "\n",
        "        return auto_keeps\n",
        "\n",
        "    def validate_formula(self, formula, X, feature_name=None, available_columns_override=None):\n",
        "        \"\"\"Comprehensively validate a feature formula with detailed error reporting\"\"\"\n",
        "\n",
        "        # Check 1: Basic validation\n",
        "        if not formula or not isinstance(formula, str):\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': 'Formula is empty or not a string',\n",
        "                'details': None\n",
        "            }\n",
        "\n",
        "        # Check 2: Dangerous operations\n",
        "        dangerous = [\n",
        "            'eval', 'exec', 'import', '__', 'open', 'file',\n",
        "            'subprocess', 'compile', 'globals', 'locals', 'vars', 'del',\n",
        "            'input', 'raw_input'\n",
        "        ]\n",
        "\n",
        "        for word in dangerous:\n",
        "            pattern = r'\\b' + re.escape(word) + r'\\b'\n",
        "            if re.search(pattern, formula):\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': f\"Contains dangerous operation: '{word}'\",\n",
        "                    'details': 'Remove dangerous operations from formula'\n",
        "                }\n",
        "\n",
        "        # Special check for 'os' and 'sys' modules\n",
        "        if re.search(r'\\bos\\s*\\.', formula) or re.search(r'\\bsys\\s*\\.', formula):\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': 'Cannot use os or sys modules',\n",
        "                'details': 'System modules are not allowed'\n",
        "            }\n",
        "\n",
        "        # Check 3: Validate column references exist\n",
        "        referenced_cols = re.findall(r\"df\\['([^']+)'\\]\", formula)\n",
        "        referenced_cols += re.findall(r'df\\[\"([^\"]+)\"\\]', formula)\n",
        "\n",
        "        if available_columns_override is not None:\n",
        "            available_columns = set(available_columns_override)\n",
        "        else:\n",
        "            available_columns = set(X.columns)\n",
        "\n",
        "        missing_cols = [col for col in referenced_cols if col not in available_columns]\n",
        "        if missing_cols:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': f\"References missing columns: {missing_cols}\",\n",
        "                'details': f\"Available columns: {list(available_columns)[:15]}\"\n",
        "            }\n",
        "\n",
        "        # Check 4: Test execution on small subset\n",
        "        try:\n",
        "            test_df = X.head(5).copy()\n",
        "            test_formula = formula.replace('df[', 'test_df[')\n",
        "            test_result = eval(test_formula)\n",
        "\n",
        "            # Validate result type\n",
        "            if not isinstance(test_result, (pd.Series, np.ndarray, int, float)):\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': f\"Formula returns invalid type: {type(test_result)}\",\n",
        "                    'details': 'Formula must return a numeric Series, array, or scalar'\n",
        "                }\n",
        "\n",
        "            # Convert to Series if needed\n",
        "            if isinstance(test_result, (int, float)):\n",
        "                test_result = pd.Series([test_result] * len(test_df))\n",
        "            elif isinstance(test_result, np.ndarray):\n",
        "                test_result = pd.Series(test_result)\n",
        "\n",
        "            # Check 5: Validate output quality\n",
        "            warning = None\n",
        "\n",
        "            if pd.isna(test_result).all():\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': 'Formula produces all NaN values',\n",
        "                    'details': 'Check for division by zero or invalid operations'\n",
        "                }\n",
        "\n",
        "            if np.isinf(test_result).any():\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': 'Formula produces infinite values',\n",
        "                    'details': 'Add small constant to denominator to avoid division by zero'\n",
        "                }\n",
        "\n",
        "            nan_rate = pd.isna(test_result).mean()\n",
        "            if nan_rate > 0.5:\n",
        "                warning = f\"Formula produces {nan_rate*100:.1f}% NaN values\"\n",
        "\n",
        "            if test_result.nunique() == 1:\n",
        "                warning = \"Formula produces constant values (zero variance)\"\n",
        "\n",
        "            return {\n",
        "                'valid': True,\n",
        "                'error': None,\n",
        "                'warning': warning,\n",
        "                'details': None\n",
        "            }\n",
        "\n",
        "        except ZeroDivisionError:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': 'Division by zero in formula',\n",
        "                'details': 'Add small constant to denominator: e.g., df[\"col\"] / (df[\"other\"] + 1e-6)'\n",
        "            }\n",
        "\n",
        "        except TypeError as e:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': f'Type error: {str(e)}',\n",
        "                'details': 'Check that all columns are numeric and operations are compatible'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': f'Execution error: {str(e)}',\n",
        "                'details': 'Check formula syntax and ensure all operations are valid'\n",
        "            }\n",
        "\n",
        "    def _create_fallback_actions(self, X, feedback):\n",
        "        \"\"\"Create safe fallback actions with explainability metadata\"\"\"\n",
        "        print(\"  üìã Creating fallback strategy...\")\n",
        "\n",
        "        fallback_actions = []\n",
        "\n",
        "        # Strategy 1: Keep top features from feedback\n",
        "        if feedback and feedback.get('top_features'):\n",
        "            for feature in feedback['top_features'][:5]:\n",
        "                if feature in X.columns:\n",
        "                    action = {\n",
        "                        \"action\": \"keep\",\n",
        "                        \"name\": feature,\n",
        "                        \"why\": \"Fallback: previous top feature\",\n",
        "                        \"explainability_score\": 3\n",
        "                    }\n",
        "\n",
        "                    # Preserve explainability if available\n",
        "                    if 'feature_explainability' in feedback and feature in feedback['feature_explainability']:\n",
        "                        exp_data = feedback['feature_explainability'][feature]\n",
        "                        action['explainability_score'] = exp_data.get('score', 3)\n",
        "                        action['physiological_rationale'] = exp_data.get('rationale', 'Previous top feature')\n",
        "\n",
        "                    fallback_actions.append(action)\n",
        "\n",
        "        # Strategy 2: Keep first few numeric columns\n",
        "        if len(fallback_actions) < 3:\n",
        "            numeric_cols = X.select_dtypes(include=[np.number]).columns[:5]\n",
        "            for col in numeric_cols:\n",
        "                if not any(a['name'] == col for a in fallback_actions):\n",
        "                    fallback_actions.append({\n",
        "                        \"action\": \"keep\",\n",
        "                        \"name\": col,\n",
        "                        \"why\": \"Fallback: numeric column preservation\",\n",
        "                        \"explainability_score\": 2,\n",
        "                        \"physiological_rationale\": \"Fallback feature - needs review\"\n",
        "                    })\n",
        "\n",
        "        return fallback_actions[:10]\n",
        "\n",
        "    def _print_validation_summary(self, valid_actions, validation_errors):\n",
        "        \"\"\"Print enhanced validation summary with explainability stats\"\"\"\n",
        "\n",
        "        action_counts = self._count_actions(valid_actions)\n",
        "\n",
        "        # Calculate explainability stats\n",
        "        create_actions = [a for a in valid_actions if a['action'] == 'create']\n",
        "\n",
        "        if create_actions:\n",
        "            scores = [a.get('explainability_score', 0) for a in create_actions]\n",
        "            avg_score = sum(scores) / len(scores) if scores else 0\n",
        "            high_quality = sum(1 for s in scores if s >= 4)\n",
        "            medium_quality = sum(1 for s in scores if 3 <= s < 4)\n",
        "            low_quality = sum(1 for s in scores if s < 3)\n",
        "\n",
        "            print(f\"  ‚úì Validated {len(valid_actions)} actions: \"\n",
        "                  f\"Keep={action_counts['keep']}, Create={action_counts['create']}, \"\n",
        "                  f\"Remove={action_counts['remove']}\")\n",
        "            print(f\"  üìä Explainability: Avg {avg_score:.1f}/5 | \"\n",
        "                  f\"High (‚â•4): {high_quality}, Medium (3): {medium_quality}, Low (<3): {low_quality}\")\n",
        "        else:\n",
        "            print(f\"  ‚úì Validated {len(valid_actions)} actions: \"\n",
        "                  f\"Keep={action_counts['keep']}, Create={action_counts['create']}, \"\n",
        "                  f\"Remove={action_counts['remove']}\")\n",
        "\n",
        "        if validation_errors:\n",
        "            print(f\"  ‚ö† Had {len(validation_errors)} validation errors (corrected via LLM retry)\")\n",
        "\n",
        "    def _count_actions(self, actions):\n",
        "        \"\"\"Count actions by type for reporting\"\"\"\n",
        "        counts = {'keep': 0, 'create': 0, 'remove': 0}\n",
        "        for action in actions:\n",
        "            action_type = action.get('action', '')\n",
        "            if action_type in counts:\n",
        "                counts[action_type] += 1\n",
        "        return counts"
      ],
      "metadata": {
        "id": "nwifPB88h90W"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureGenerator:\n",
        "    \"\"\"Generates features based on validated strategies\"\"\"\n",
        "\n",
        "    def generate(self, X, agent_result):\n",
        "        \"\"\"Generate features with dependency resolution and multi-pass creation\"\"\"\n",
        "        X_new = X.copy()\n",
        "\n",
        "        # Track what we're doing (including explainability)\n",
        "        kept_features = set()\n",
        "        created_features = set()\n",
        "        removed_features = set()\n",
        "        failed_features = []\n",
        "        feature_metadata = {}  # NEW: Track explainability metadata\n",
        "\n",
        "        print(\"\\n  üîß Executing feature actions:\")\n",
        "\n",
        "        # Extract validation errors if present\n",
        "        validation_errors = agent_result.get('validation_errors', [])\n",
        "        if validation_errors:\n",
        "            print(f\"  ‚ÑπÔ∏è  Note: {len(validation_errors)} actions were filtered during validation\")\n",
        "\n",
        "        # Step 1: Process KEEP actions first\n",
        "        keep_actions = [a for a in agent_result.get('actions', []) if a.get('action') == 'keep']\n",
        "        for action in keep_actions:\n",
        "            name = action.get('name')\n",
        "            if not name:\n",
        "                continue\n",
        "\n",
        "            if name in kept_features:\n",
        "                print(f\"  ‚ö† Duplicate keep for '{name}' - skipping\")\n",
        "                continue\n",
        "\n",
        "            if name in X_new.columns:\n",
        "                kept_features.add(name)\n",
        "                # NEW: Preserve explainability metadata\n",
        "                if 'explainability_score' in action:\n",
        "                    feature_metadata[name] = {\n",
        "                        'explainability_score': action.get('explainability_score'),\n",
        "                        'physiological_rationale': action.get('physiological_rationale'),\n",
        "                        'action_type': 'keep'\n",
        "                    }\n",
        "                print(f\"  ‚úì Keeping: {name}\")\n",
        "            else:\n",
        "                print(f\"  ‚ö† Cannot keep '{name}' - doesn't exist\")\n",
        "\n",
        "        # Step 2: Process CREATE actions with dependency resolution\n",
        "        create_actions = [a for a in agent_result.get('actions', []) if a.get('action') == 'create']\n",
        "        max_passes = 5\n",
        "\n",
        "        for pass_num in range(max_passes):\n",
        "            if not create_actions:\n",
        "                break\n",
        "\n",
        "            if pass_num > 0:\n",
        "                print(f\"\\n  üîÑ Pass {pass_num + 1}: Resolving dependencies for {len(create_actions)} features...\")\n",
        "\n",
        "            remaining_actions = []\n",
        "            created_this_pass = 0\n",
        "\n",
        "            for action in create_actions:\n",
        "                name = action.get('name')\n",
        "                formula = action.get('formula', '')\n",
        "\n",
        "                if name in created_features:\n",
        "                    continue\n",
        "\n",
        "                # Extract dependencies\n",
        "                import re\n",
        "                referenced_cols = re.findall(r\"df\\['([^']+)'\\]\", formula)\n",
        "                referenced_cols += re.findall(r'df\\[\"([^\"]+)\"\\]', formula)\n",
        "\n",
        "                missing = [col for col in referenced_cols if col not in X_new.columns]\n",
        "\n",
        "                if missing:\n",
        "                    remaining_actions.append(action)\n",
        "                    if pass_num == max_passes - 1:\n",
        "                        print(f\"  ‚ùå Cannot create '{name}' - missing columns: {missing}\")\n",
        "                        failed_features.append({\n",
        "                            'name': name,\n",
        "                            'missing': missing,\n",
        "                            'formula': formula\n",
        "                        })\n",
        "                    continue\n",
        "\n",
        "                # Try to create\n",
        "                try:\n",
        "                    X_new[name] = eval(formula)\n",
        "                    created_features.add(name)\n",
        "                    created_this_pass += 1\n",
        "\n",
        "                    # NEW: Store explainability metadata\n",
        "                    feature_metadata[name] = {\n",
        "                        'explainability_score': action.get('explainability_score'),\n",
        "                        'physiological_rationale': action.get('physiological_rationale'),\n",
        "                        'clinical_relevance': action.get('clinical_relevance'),\n",
        "                        'interpretation': action.get('interpretation'),\n",
        "                        'action_type': 'create',\n",
        "                        'formula': formula\n",
        "                    }\n",
        "\n",
        "                    exp_score = action.get('explainability_score', '?')\n",
        "                    print(f\"  ‚úì Created: {name} (Explainability: {exp_score}/5)\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Failed to create '{name}': {e}\")\n",
        "                    failed_features.append({\n",
        "                        'name': name,\n",
        "                        'error': str(e),\n",
        "                        'formula': formula\n",
        "                    })\n",
        "\n",
        "            create_actions = remaining_actions\n",
        "\n",
        "            if created_this_pass == 0 and create_actions:\n",
        "                print(f\"  ‚ö† No progress in pass {pass_num + 1}, stopping early\")\n",
        "                for action in create_actions:\n",
        "                    name = action.get('name')\n",
        "                    formula = action.get('formula', '')\n",
        "                    import re\n",
        "                    referenced_cols = re.findall(r\"df\\['([^']+)'\\]\", formula)\n",
        "                    referenced_cols += re.findall(r'df\\[\"([^\"]+)\"\\]', formula)\n",
        "                    missing = [col for col in referenced_cols if col not in X_new.columns]\n",
        "                    print(f\"  ‚ùå Circular dependency or missing: '{name}' needs {missing}\")\n",
        "                break\n",
        "\n",
        "        # Step 3: Process REMOVE actions\n",
        "        remove_actions = [a for a in agent_result.get('actions', []) if a.get('action') == 'remove']\n",
        "        for action in remove_actions:\n",
        "            name = action.get('name')\n",
        "            if not name:\n",
        "                continue\n",
        "\n",
        "            if name in kept_features:\n",
        "                print(f\"  ‚ö† Skipping removal of '{name}' - explicitly kept\")\n",
        "                continue\n",
        "            if name in created_features:\n",
        "                print(f\"  ‚ö† Skipping removal of '{name}' - just created\")\n",
        "                continue\n",
        "\n",
        "            removed_features.add(name)\n",
        "\n",
        "        for name in removed_features:\n",
        "            if name in X_new.columns:\n",
        "                X_new.drop(columns=[name], inplace=True)\n",
        "                print(f\"  ‚úì Removed: {name}\")\n",
        "\n",
        "        # Step 4: Determine final feature set\n",
        "        if kept_features or created_features:\n",
        "            final_features = kept_features.union(created_features)\n",
        "            available_finals = [f for f in final_features if f in X_new.columns]\n",
        "            X_new = X_new[available_finals]\n",
        "        else:\n",
        "            print(\"  ‚ÑπÔ∏è No features explicitly kept - preserving all original features\")\n",
        "\n",
        "        # Step 5: Quality checks\n",
        "        print(\"\\n  üîç Final data quality check:\")\n",
        "        X_new = self._quality_check(X_new, len(X.columns))\n",
        "\n",
        "        # Step 6: Add metadata to result\n",
        "        result = {\n",
        "            'X_new': X_new,\n",
        "            'feature_metadata': feature_metadata,  # NEW\n",
        "            'summary': {\n",
        "                'input_features': len(X.columns),\n",
        "                'output_features': len(X_new.columns),\n",
        "                'kept': len(kept_features),\n",
        "                'created': len(created_features),\n",
        "                'removed': len(removed_features),\n",
        "                'failed': len(failed_features)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n  üìä Feature Generation Summary:\")\n",
        "        print(f\"     Input features:   {len(X.columns)}\")\n",
        "        print(f\"     Output features:  {len(X_new.columns)}\")\n",
        "        print(f\"     Kept:             {len(kept_features)}\")\n",
        "        print(f\"     Created:          {len(created_features)}\")\n",
        "        print(f\"     Removed:          {len(removed_features)}\")\n",
        "        print(f\"     Failed:           {len(failed_features)}\")\n",
        "\n",
        "        if failed_features and len(failed_features) <= 5:\n",
        "            print(f\"\\n  ‚ö† Failed features:\")\n",
        "            for fail in failed_features[:5]:\n",
        "                if 'missing' in fail:\n",
        "                    print(f\"     - {fail['name']}: missing {fail['missing']}\")\n",
        "                else:\n",
        "                    print(f\"     - {fail['name']}: {fail.get('error', 'unknown error')}\")\n",
        "\n",
        "        return result  # NEW: Return dict with metadata instead of just X_new\n",
        "\n",
        "    def _quality_check(self, X_new, original_count):\n",
        "        \"\"\"Ensure the new feature set is valid\"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        # Check 1: Not empty\n",
        "        if X_new.empty or len(X_new.columns) == 0:\n",
        "            raise ValueError(\"Feature generation resulted in empty dataframe!\")\n",
        "\n",
        "        # Check 2: No duplicate columns\n",
        "        if len(X_new.columns) != len(set(X_new.columns)):\n",
        "            duplicates = [col for col in X_new.columns if list(X_new.columns).count(col) > 1]\n",
        "            print(f\"    ‚ö† Removing duplicate columns: {set(duplicates)}\")\n",
        "            X_new = X_new.loc[:, ~X_new.columns.duplicated()]\n",
        "\n",
        "        # Check 3: Remove all-NaN columns\n",
        "        all_nan_cols = X_new.columns[X_new.isna().all()].tolist()\n",
        "        if all_nan_cols:\n",
        "            print(f\"    ‚ö† Removing all-NaN columns: {all_nan_cols}\")\n",
        "            X_new = X_new.drop(columns=all_nan_cols)\n",
        "\n",
        "        # Check 4: Remove constant columns (zero variance)\n",
        "        constant_cols = []\n",
        "        for col in X_new.columns:\n",
        "            if X_new[col].dtype in [np.number]:\n",
        "                if X_new[col].nunique() == 1:\n",
        "                    constant_cols.append(col)\n",
        "\n",
        "        if constant_cols:\n",
        "            print(f\"    ‚ö† Removing constant columns: {constant_cols}\")\n",
        "            X_new = X_new.drop(columns=constant_cols)\n",
        "\n",
        "        # Check 5: Reasonable feature count\n",
        "        if len(X_new.columns) > original_count * 5:\n",
        "            print(f\"    ‚ö† WARNING: Feature count exploded: {original_count} ‚Üí {len(X_new.columns)}\")\n",
        "            print(f\"       This might cause overfitting!\")\n",
        "\n",
        "        print(f\"    ‚úì Passed quality checks\")\n",
        "\n",
        "        return X_new"
      ],
      "metadata": {
        "id": "cekcTQlkiFEd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def best_threshold_for_f1(y_true, y_prob):\n",
        "    \"\"\"\n",
        "    Find threshold maximizing F1-score for given probabilities.\n",
        "    Returns (best_threshold, best_f1, precision, recall).\n",
        "    \"\"\"\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "    f1s = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "    idx = np.nanargmax(f1s)\n",
        "    return thresholds[idx], f1s[idx], precision[idx], recall[idx]"
      ],
      "metadata": {
        "id": "Sc67_bK4Q75l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEvaluator:\n",
        "    \"\"\"Evaluates feature sets using explainable models with comprehensive metrics\"\"\"\n",
        "\n",
        "    def __init__(self, task_type='classification', models=None, n_cv_folds=5,\n",
        "                 preprocessing='auto', random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize evaluator with explainable models and detailed metrics\n",
        "\n",
        "        Args:\n",
        "            task_type: 'classification' or 'regression'\n",
        "            models: List of model names. If None, uses explainable defaults.\n",
        "                   Options: ['random_forest', 'xgboost', 'gradient_boosting', 'svm',\n",
        "                            'decision_tree', 'extra_trees']\n",
        "            n_cv_folds: Number of cross-validation folds\n",
        "            preprocessing: 'auto', 'standard', 'minmax', 'robust', or None\n",
        "            random_state: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.task_type = task_type\n",
        "        self.n_cv_folds = n_cv_folds\n",
        "        self.preprocessing = preprocessing\n",
        "        self.random_state = random_state\n",
        "\n",
        "        # Set default to EXPLAINABLE models only\n",
        "        if models is None:\n",
        "            self.models = ['random_forest', 'xgboost', 'gradient_boosting', 'svm']\n",
        "        else:\n",
        "            self.models = models\n",
        "\n",
        "        self.scaler = None\n",
        "        self.imputer = None\n",
        "\n",
        "    def evaluate(self, X, y, feature_metadata=None, return_all_scores=False):\n",
        "        \"\"\"\n",
        "        Evaluate features with multiple explainable models and detailed metrics\n",
        "\n",
        "        Args:\n",
        "            X: Feature dataframe\n",
        "            y: Target variable\n",
        "            feature_metadata: Dict with explainability scores from FeatureGenerator\n",
        "            return_all_scores: If True, return detailed metrics per model\n",
        "\n",
        "        Returns:\n",
        "            avg_score: Average primary metric across all models\n",
        "            top_features: List of most important features\n",
        "            detailed_metrics: (optional) Comprehensive metrics if return_all_scores=True\n",
        "        \"\"\"\n",
        "        from sklearn.model_selection import cross_val_score, cross_validate\n",
        "        import numpy as np\n",
        "\n",
        "        # Clean and preprocess data\n",
        "        X_cleaned = self._clean_data(X)\n",
        "        X_processed = self._preprocess_data(X_cleaned)\n",
        "\n",
        "        self._spw = None\n",
        "        if self.task_type == 'classification':\n",
        "            classes = np.unique(y)\n",
        "            if len(classes) == 2:\n",
        "                n_pos = np.sum(y == classes[1])\n",
        "                n_neg = np.sum(y == classes[0])\n",
        "                if n_pos > 0:\n",
        "                    self._spw = float(n_neg) / float(n_pos)\n",
        "\n",
        "        # Track results\n",
        "        model_scores = {}\n",
        "        all_importances = {}\n",
        "        detailed_metrics = {}\n",
        "\n",
        "        print(f\"\\n  üìä Evaluating with {len(self.models)} explainable models ({self.n_cv_folds}-fold CV):\")\n",
        "\n",
        "        for model_name in self.models:\n",
        "            try:\n",
        "                model = self._get_model(model_name)\n",
        "\n",
        "                # Get comprehensive metrics using cross_validate\n",
        "                scoring_metrics = self._get_scoring_metrics()\n",
        "\n",
        "                cv_results = cross_validate(\n",
        "                    model, X_processed, y,\n",
        "                    cv=self.n_cv_folds,\n",
        "                    scoring=scoring_metrics,\n",
        "                    return_train_score=False\n",
        "                )\n",
        "\n",
        "                # Extract primary metric\n",
        "                primary_metric = self._get_primary_metric_name()\n",
        "                primary_scores = cv_results[f'test_{primary_metric}']\n",
        "                avg_score = primary_scores.mean()\n",
        "                std_score = primary_scores.std()\n",
        "\n",
        "                model_scores[model_name] = {\n",
        "                    'mean': avg_score,\n",
        "                    'std': std_score,\n",
        "                    'scores': primary_scores\n",
        "                }\n",
        "\n",
        "                # Store detailed metrics\n",
        "                detailed_metrics[model_name] = self._extract_detailed_metrics(cv_results)\n",
        "\n",
        "                # Print summary with explainability note\n",
        "                print(f\"    {model_name:20s}: {avg_score:.4f} (¬±{std_score:.4f}) \"\n",
        "                      f\"[Explainable ‚úì]\")\n",
        "\n",
        "                # Get feature importances\n",
        "                importances = self._get_feature_importance(model, X_processed, y)\n",
        "                if importances is not None:\n",
        "                    all_importances[model_name] = importances\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö† {model_name} failed: {e}\")\n",
        "                model_scores[model_name] = {'mean': 0.0, 'std': 0.0, 'scores': []}\n",
        "\n",
        "        # Aggregate results\n",
        "        if not model_scores or all(s['mean'] == 0.0 for s in model_scores.values()):\n",
        "            print(\"  ‚ö† All models failed! Returning fallback values.\")\n",
        "            return 0.0, list(X_cleaned.columns[:5]), {}\n",
        "\n",
        "        # Calculate average score across models\n",
        "        valid_scores = [s['mean'] for s in model_scores.values() if s['mean'] > 0]\n",
        "        avg_score = np.mean(valid_scores) if valid_scores else 0.0\n",
        "\n",
        "        # Aggregate feature importances with explainability weighting\n",
        "        top_features = self._aggregate_feature_importance(\n",
        "            all_importances,\n",
        "            X_processed.columns,\n",
        "            feature_metadata\n",
        "        )\n",
        "\n",
        "        # Print summary with detailed metrics\n",
        "        print(f\"\\n  ‚úì Average {self._get_primary_metric_name()}: {avg_score:.4f}\")\n",
        "        self._print_detailed_metrics_summary(detailed_metrics)\n",
        "        print(f\"  ‚úì Top features: {top_features[:5]}\")\n",
        "\n",
        "        if return_all_scores:\n",
        "            return avg_score, top_features, {\n",
        "                'model_scores': model_scores,\n",
        "                'detailed_metrics': detailed_metrics,\n",
        "                'feature_importances': all_importances\n",
        "            }\n",
        "        else:\n",
        "            return avg_score, top_features\n",
        "\n",
        "    def _get_model(self, model_name):\n",
        "        \"\"\"Get explainable model instance by name\"\"\"\n",
        "        from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
        "                                       GradientBoostingClassifier, GradientBoostingRegressor,\n",
        "                                       ExtraTreesClassifier, ExtraTreesRegressor)\n",
        "        from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "        from sklearn.svm import SVC, SVR\n",
        "\n",
        "        # Try to import optional libraries\n",
        "        try:\n",
        "            from xgboost import XGBClassifier, XGBRegressor\n",
        "            has_xgboost = True\n",
        "        except ImportError:\n",
        "            has_xgboost = False\n",
        "\n",
        "        is_classification = self.task_type == 'classification'\n",
        "\n",
        "        # EXPLAINABLE MODELS ONLY\n",
        "        if model_name == 'random_forest':\n",
        "            if is_classification:\n",
        "                return RandomForestClassifier(\n",
        "                n_estimators=400,\n",
        "                max_depth=12,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                class_weight='balanced',  # ‚Üê add this\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            else:\n",
        "                return RandomForestRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=10,\n",
        "                    min_samples_split=5,\n",
        "                    min_samples_leaf=2,\n",
        "                    random_state=self.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "\n",
        "        elif model_name == 'xgboost':\n",
        "            if not has_xgboost:\n",
        "                raise ImportError(\"XGBoost not installed. Install with: pip install xgboost\")\n",
        "\n",
        "            if is_classification:\n",
        "                return XGBClassifier(\n",
        "                n_estimators=600,\n",
        "                max_depth=4,\n",
        "                learning_rate=0.05,\n",
        "                subsample=0.9,\n",
        "                colsample_bytree=0.8,\n",
        "                random_state=self.random_state,\n",
        "                scale_pos_weight=(self._spw if getattr(self, \"_spw\", None) else 1.0),  # ‚Üê add this\n",
        "                eval_metric='aucpr'  # ‚Üê focus on PR\n",
        "            )\n",
        "            else:\n",
        "                return XGBRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=self.random_state,\n",
        "                    verbosity=0\n",
        "                )\n",
        "\n",
        "        elif model_name == 'gradient_boosting':\n",
        "            # Scikit-learn's GradientBoosting - highly explainable\n",
        "            if is_classification:\n",
        "                return GradientBoostingClassifier(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=5,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=self.random_state\n",
        "                )\n",
        "            else:\n",
        "                return GradientBoostingRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=5,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=self.random_state\n",
        "                )\n",
        "\n",
        "        elif model_name == 'extra_trees':\n",
        "            # Extra Trees - similar to RF but different splitting strategy\n",
        "            if is_classification:\n",
        "                return ExtraTreesClassifier(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=10,\n",
        "                    min_samples_split=5,\n",
        "                    random_state=self.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "            else:\n",
        "                return ExtraTreesRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=10,\n",
        "                    min_samples_split=5,\n",
        "                    random_state=self.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "\n",
        "        elif model_name == 'decision_tree':\n",
        "            # Single decision tree - maximally explainable\n",
        "            if is_classification:\n",
        "                return DecisionTreeClassifier(\n",
        "                    max_depth=8,\n",
        "                    min_samples_split=10,\n",
        "                    min_samples_leaf=5,\n",
        "                    random_state=self.random_state\n",
        "                )\n",
        "            else:\n",
        "                return DecisionTreeRegressor(\n",
        "                    max_depth=8,\n",
        "                    min_samples_split=10,\n",
        "                    min_samples_leaf=5,\n",
        "                    random_state=self.random_state\n",
        "                )\n",
        "\n",
        "        elif model_name == 'svm':\n",
        "            # SVM requires scaling\n",
        "            if is_classification:\n",
        "                return SVC(\n",
        "                    kernel='rbf',\n",
        "                    C=1.0,\n",
        "                    random_state=self.random_state,\n",
        "                    max_iter=1000,\n",
        "                    probability=True  # For ROC-AUC\n",
        "                )\n",
        "            else:\n",
        "                return SVR(\n",
        "                    kernel='rbf',\n",
        "                    C=1.0,\n",
        "                    max_iter=1000\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model: {model_name}. Use explainable models only.\")\n",
        "\n",
        "    def _get_scoring_metrics(self):\n",
        "      if self.task_type == 'classification':\n",
        "        return {\n",
        "            'average_precision': 'average_precision',  # PRIMARY (AUCPR)\n",
        "            'roc_auc': 'roc_auc',\n",
        "            'accuracy': 'accuracy',\n",
        "            'precision': 'precision_macro',\n",
        "            'recall': 'recall_macro',\n",
        "            'f1': 'f1_macro',\n",
        "        }\n",
        "      else:\n",
        "            return {\n",
        "                'r2': 'r2',\n",
        "                'neg_mse': 'neg_mean_squared_error',\n",
        "                'neg_mae': 'neg_mean_absolute_error',\n",
        "                'neg_rmse': 'neg_root_mean_squared_error'\n",
        "            }\n",
        "\n",
        "    def _get_primary_metric_name(self):\n",
        "      return 'average_precision' if self.task_type == 'classification' else 'r2'\n",
        "\n",
        "    def _extract_detailed_metrics(self, cv_results):\n",
        "        \"\"\"Extract and organize detailed metrics from cross-validation\"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        for key, values in cv_results.items():\n",
        "            if key.startswith('test_'):\n",
        "                metric_name = key.replace('test_', '')\n",
        "                metrics[metric_name] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values),\n",
        "                    'values': values\n",
        "                }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _print_detailed_metrics_summary(self, detailed_metrics):\n",
        "        \"\"\"Print summary of detailed metrics across all models\"\"\"\n",
        "        if not detailed_metrics:\n",
        "            return\n",
        "\n",
        "        print(\"\\n  üìà Detailed Performance Metrics (averaged across models):\")\n",
        "\n",
        "        # Aggregate metrics across models\n",
        "        all_metric_names = set()\n",
        "        for model_metrics in detailed_metrics.values():\n",
        "            all_metric_names.update(model_metrics.keys())\n",
        "\n",
        "        aggregated = {}\n",
        "        for metric_name in all_metric_names:\n",
        "            values = []\n",
        "            for model_metrics in detailed_metrics.values():\n",
        "                if metric_name in model_metrics:\n",
        "                    values.append(model_metrics[metric_name]['mean'])\n",
        "\n",
        "            if values:\n",
        "                aggregated[metric_name] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values)\n",
        "                }\n",
        "\n",
        "        # Print classification metrics\n",
        "        if self.task_type == 'classification':\n",
        "            if 'accuracy' in aggregated:\n",
        "                print(f\"    Accuracy:  {aggregated['accuracy']['mean']:.4f} (¬±{aggregated['accuracy']['std']:.4f})\")\n",
        "            if 'precision' in aggregated:\n",
        "                print(f\"    Precision: {aggregated['precision']['mean']:.4f} (¬±{aggregated['precision']['std']:.4f}) \"\n",
        "                      \"[True Pos / (True Pos + False Pos)]\")\n",
        "            if 'recall' in aggregated:\n",
        "                print(f\"    Recall:    {aggregated['recall']['mean']:.4f} (¬±{aggregated['recall']['std']:.4f}) \"\n",
        "                      \"[True Pos / (True Pos + False Neg)]\")\n",
        "            if 'f1' in aggregated:\n",
        "                print(f\"    F1 Score:  {aggregated['f1']['mean']:.4f} (¬±{aggregated['f1']['std']:.4f}) \"\n",
        "                      \"[Harmonic mean of precision & recall]\")\n",
        "            if 'roc_auc' in aggregated:\n",
        "                print(f\"    ROC-AUC:   {aggregated['roc_auc']['mean']:.4f} (¬±{aggregated['roc_auc']['std']:.4f})\")\n",
        "\n",
        "        # Print regression metrics\n",
        "        else:\n",
        "            if 'r2' in aggregated:\n",
        "                print(f\"    R¬≤ Score:  {aggregated['r2']['mean']:.4f} (¬±{aggregated['r2']['std']:.4f})\")\n",
        "            if 'neg_mae' in aggregated:\n",
        "                mae = -aggregated['neg_mae']['mean']  # Convert back to positive\n",
        "                print(f\"    MAE:       {mae:.4f} (¬±{aggregated['neg_mae']['std']:.4f})\")\n",
        "            if 'neg_rmse' in aggregated:\n",
        "                rmse = -aggregated['neg_rmse']['mean']\n",
        "                print(f\"    RMSE:      {rmse:.4f} (¬±{aggregated['neg_rmse']['std']:.4f})\")\n",
        "\n",
        "    def _clean_data(self, X):\n",
        "        \"\"\"Clean data: handle inf, NaN, duplicates, constants\"\"\"\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "\n",
        "        X_cleaned = X.copy()\n",
        "\n",
        "        print(\"  üßπ Cleaning data...\")\n",
        "\n",
        "        # Step 1: Handle infinite values\n",
        "        inf_cols = X_cleaned.columns[np.isinf(X_cleaned).any()].tolist()\n",
        "        if inf_cols:\n",
        "            print(f\"    ‚ö† Replacing inf values in {len(inf_cols)} columns\")\n",
        "            X_cleaned = X_cleaned.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Step 2: Check for constant columns (zero variance)\n",
        "        constant_cols = []\n",
        "        for col in X_cleaned.columns:\n",
        "            if X_cleaned[col].nunique() <= 1:\n",
        "                constant_cols.append(col)\n",
        "\n",
        "        if constant_cols:\n",
        "            print(f\"    ‚ö† Dropping {len(constant_cols)} constant columns: {constant_cols[:5]}\")\n",
        "            X_cleaned = X_cleaned.drop(columns=constant_cols)\n",
        "\n",
        "        # Step 3: Check for duplicate columns\n",
        "        duplicate_cols = []\n",
        "        for i, col1 in enumerate(X_cleaned.columns):\n",
        "            for col2 in X_cleaned.columns[i+1:]:\n",
        "                if X_cleaned[col1].equals(X_cleaned[col2]):\n",
        "                    duplicate_cols.append(col2)\n",
        "\n",
        "        if duplicate_cols:\n",
        "            print(f\"    ‚ö† Dropping {len(duplicate_cols)} duplicate columns\")\n",
        "            X_cleaned = X_cleaned.drop(columns=duplicate_cols)\n",
        "\n",
        "        # Step 4: Handle categorical columns\n",
        "        categorical_cols = X_cleaned.select_dtypes(include=['object', 'category']).columns\n",
        "        if len(categorical_cols) > 0:\n",
        "            print(f\"    ‚ö† Converting {len(categorical_cols)} categorical columns to numeric\")\n",
        "            from sklearn.preprocessing import LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            for col in categorical_cols:\n",
        "                try:\n",
        "                    X_cleaned[col] = le.fit_transform(X_cleaned[col].astype(str))\n",
        "                except:\n",
        "                    print(f\"      ‚ö† Failed to encode {col}, dropping it\")\n",
        "                    X_cleaned = X_cleaned.drop(columns=[col])\n",
        "\n",
        "        print(f\"    ‚úì Cleaned: {X.shape[1]} ‚Üí {X_cleaned.shape[1]} features\")\n",
        "\n",
        "        return X_cleaned\n",
        "\n",
        "    def _preprocess_data(self, X):\n",
        "        \"\"\"Preprocess data with imputation and scaling\"\"\"\n",
        "        from sklearn.impute import SimpleImputer\n",
        "        from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "\n",
        "        print(\"  ‚öôÔ∏è Preprocessing data...\")\n",
        "\n",
        "        # Step 1: Imputation\n",
        "        if X.isna().any().any():\n",
        "            nan_counts = X.isna().sum()\n",
        "            nan_cols = nan_counts[nan_counts > 0]\n",
        "            print(f\"    Imputing NaN in {len(nan_cols)} columns\")\n",
        "\n",
        "            self.imputer = SimpleImputer(strategy='median')\n",
        "            X_imputed = pd.DataFrame(\n",
        "                self.imputer.fit_transform(X),\n",
        "                columns=X.columns,\n",
        "                index=X.index\n",
        "            )\n",
        "        else:\n",
        "            X_imputed = X.copy()\n",
        "\n",
        "        # Step 2: Scaling (important for SVM)\n",
        "        if self.preprocessing is None:\n",
        "            print(\"    No scaling applied\")\n",
        "            return X_imputed\n",
        "\n",
        "        # Auto-detect best scaler\n",
        "        if self.preprocessing == 'auto':\n",
        "            has_outliers = False\n",
        "            for col in X_imputed.columns:\n",
        "                q1, q3 = X_imputed[col].quantile([0.25, 0.75])\n",
        "                iqr = q3 - q1\n",
        "                if iqr > 0:\n",
        "                    outlier_range = (X_imputed[col].max() - X_imputed[col].min()) / iqr\n",
        "                    if outlier_range > 10:\n",
        "                        has_outliers = True\n",
        "                        break\n",
        "\n",
        "            scaler_type = 'robust' if has_outliers else 'standard'\n",
        "            print(f\"    Auto-selected {scaler_type} scaling\")\n",
        "        else:\n",
        "            scaler_type = self.preprocessing\n",
        "\n",
        "        # Apply scaling\n",
        "        if scaler_type == 'standard':\n",
        "            self.scaler = StandardScaler()\n",
        "        elif scaler_type == 'minmax':\n",
        "            self.scaler = MinMaxScaler()\n",
        "        elif scaler_type == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "        else:\n",
        "            print(f\"    ‚ö† Unknown scaler: {scaler_type}, using standard\")\n",
        "            self.scaler = StandardScaler()\n",
        "\n",
        "        X_scaled = pd.DataFrame(\n",
        "            self.scaler.fit_transform(X_imputed),\n",
        "            columns=X_imputed.columns,\n",
        "            index=X_imputed.index\n",
        "        )\n",
        "\n",
        "        print(f\"    ‚úì Applied {scaler_type} scaling\")\n",
        "\n",
        "        return X_scaled\n",
        "\n",
        "    def _get_feature_importance(self, model, X, y):\n",
        "        \"\"\"Extract feature importance from model\"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        # Fit model if not already fitted\n",
        "        if not hasattr(model, 'feature_importances_') and not hasattr(model, 'coef_'):\n",
        "            try:\n",
        "                model.fit(X, y)\n",
        "            except:\n",
        "                return None\n",
        "\n",
        "        # Get importances\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            return dict(zip(X.columns, model.feature_importances_))\n",
        "        elif hasattr(model, 'coef_'):\n",
        "            # For SVM with linear kernel or other linear models\n",
        "            coef = model.coef_\n",
        "            if len(coef.shape) > 1:  # Multi-class\n",
        "                coef = np.abs(coef).mean(axis=0)\n",
        "            else:\n",
        "                coef = np.abs(coef)\n",
        "            return dict(zip(X.columns, coef))\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _aggregate_feature_importance(self, all_importances, feature_names, feature_metadata=None):\n",
        "        \"\"\"\n",
        "        Aggregate feature importance across models with explainability weighting\n",
        "\n",
        "        Features with higher explainability scores get a small boost in ranking\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        if not all_importances:\n",
        "            return list(feature_names[:10])\n",
        "\n",
        "        # Get explainability scores if available\n",
        "        explainability_scores = {}\n",
        "        if feature_metadata:\n",
        "            for feat in feature_names:\n",
        "                if feat in feature_metadata:\n",
        "                    explainability_scores[feat] = feature_metadata[feat].get('explainability_score', 3)\n",
        "                else:\n",
        "                    explainability_scores[feat] = 3  # Default\n",
        "\n",
        "        # Rank features for each model\n",
        "        feature_ranks = {feat: [] for feat in feature_names}\n",
        "\n",
        "        for model_name, importances in all_importances.items():\n",
        "            # Sort by importance\n",
        "            sorted_features = sorted(importances.items(), key=lambda x: -x[1])\n",
        "\n",
        "            # Assign ranks\n",
        "            for rank, (feat, _) in enumerate(sorted_features, 1):\n",
        "                feature_ranks[feat].append(rank)\n",
        "\n",
        "        # Average ranks with optional explainability weighting\n",
        "        avg_ranks = {}\n",
        "        for feat, ranks in feature_ranks.items():\n",
        "            if ranks:\n",
        "                base_rank = np.mean(ranks)\n",
        "\n",
        "                # Apply small explainability bonus (10% max)\n",
        "                if feat in explainability_scores:\n",
        "                    exp_score = explainability_scores[feat]\n",
        "                    # Higher explainability = lower (better) rank\n",
        "                    explainability_bonus = (5 - exp_score) * 0.02 * base_rank\n",
        "                    avg_ranks[feat] = base_rank + explainability_bonus\n",
        "                else:\n",
        "                    avg_ranks[feat] = base_rank\n",
        "            else:\n",
        "                avg_ranks[feat] = len(feature_names)\n",
        "\n",
        "        # Sort by average rank (lower is better)\n",
        "        top_features = sorted(avg_ranks.items(), key=lambda x: x[1])\n",
        "\n",
        "        return [f[0] for f in top_features[:10]]"
      ],
      "metadata": {
        "id": "iNSMWlpdiJEE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#patch class\n",
        "\n",
        "def _extract_explainability_scores(feature_metadata):\n",
        "    \"\"\"\n",
        "    Return a list of numeric 'score' values from feature_metadata in a tolerant way.\n",
        "    Accepts dicts of dicts, dicts of numbers, or lists of either.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if feature_metadata is None:\n",
        "        return out\n",
        "\n",
        "    def _maybe_add(x):\n",
        "        try:\n",
        "            val = float(x)\n",
        "            if math.isfinite(val):\n",
        "                out.append(val)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Case: dict-like\n",
        "    if isinstance(feature_metadata, dict):\n",
        "        for v in feature_metadata.values():\n",
        "            if isinstance(v, dict):\n",
        "                s = v.get('score')\n",
        "                if isinstance(s, (int, float)):\n",
        "                    _maybe_add(s)\n",
        "            elif isinstance(v, (int, float)):\n",
        "                _maybe_add(v)\n",
        "            # else: ignore strings/None/etc.\n",
        "\n",
        "    # Case: list-like\n",
        "    elif isinstance(feature_metadata, (list, tuple)):\n",
        "        for v in feature_metadata:\n",
        "            if isinstance(v, dict):\n",
        "                s = v.get('score')\n",
        "                if isinstance(s, (int, float)):\n",
        "                    _maybe_add(s)\n",
        "            elif isinstance(v, (int, float)):\n",
        "                _maybe_add(v)\n",
        "\n",
        "    # Anything else: ignore\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "xQKnOTdAPhWP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineeringPipeline:\n",
        "    \"\"\"Main pipeline that orchestrates all components with explainability tracking\"\"\"\n",
        "\n",
        "    def __init__(self, api_key, task_type='classification',\n",
        "             models=None, preprocessing='auto', n_cv_folds=5,\n",
        "             min_explainability_score=3, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the feature engineering pipeline\n",
        "\n",
        "        Args:\n",
        "            api_key: OpenAI API key\n",
        "            task_type: 'classification' or 'regression'\n",
        "            models: List of models for evaluation\n",
        "            preprocessing: Preprocessing strategy\n",
        "            n_cv_folds: Number of cross-validation folds\n",
        "            min_explainability_score: Minimum explainability score (1-5)\n",
        "            random_state: Random seed\n",
        "        \"\"\"\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.task_type = task_type\n",
        "        self.random_state = random_state\n",
        "        self.history = []\n",
        "\n",
        "        # Track best results\n",
        "        self.best_model = None\n",
        "        self.best_X = None\n",
        "        self.best_y = None\n",
        "        self.feature_formulas = {}\n",
        "        self.feature_metadata = {}\n",
        "\n",
        "        # Initialize components\n",
        "        self.researcher = ResearchAgent(self.client)  # No serper key needed\n",
        "        self.validator = FeatureValidator(\n",
        "            client=self.client,\n",
        "            min_explainability_score=min_explainability_score\n",
        "        )\n",
        "        self.strategist = FeatureStrategyAgent(self.client, validator=self.validator)\n",
        "        self.generator = FeatureGenerator()\n",
        "        self.evaluator = FeatureEvaluator(\n",
        "            task_type=task_type,\n",
        "            models=models,\n",
        "            preprocessing=preprocessing,\n",
        "            n_cv_folds=n_cv_folds,\n",
        "            random_state=random_state\n",
        "        )\n",
        "    def _safe_scores_from_metadata(self, feature_metadata):\n",
        "      vals = {}\n",
        "      if not feature_metadata:\n",
        "          return vals\n",
        "      if isinstance(feature_metadata, dict):\n",
        "          for k, v in feature_metadata.items():\n",
        "              if isinstance(v, dict) and 'score' in v and isinstance(v['score'], (int, float)):\n",
        "                  vals[k] = float(v['score'])\n",
        "              elif isinstance(v, (int, float)):\n",
        "                  vals[k] = float(v)\n",
        "      elif isinstance(feature_metadata, (list, tuple)):\n",
        "          # optional: support list forms\n",
        "          for item in feature_metadata:\n",
        "              if isinstance(item, dict):\n",
        "                  name = item.get('name')\n",
        "                  s = item.get('score')\n",
        "                  if name and isinstance(s, (int, float)):\n",
        "                      vals[name] = float(s)\n",
        "      return vals\n",
        "    def run(self, df, target_col, max_iterations=30, min_improvement=0.01,\n",
        "            patience=3, metadata=None, verbose=True):\n",
        "        \"\"\"\n",
        "        Run the feature engineering pipeline\n",
        "\n",
        "        Args:\n",
        "            df: Input dataframe\n",
        "            target_col: Name of target column\n",
        "            max_iterations: Maximum number of iterations\n",
        "            min_improvement: Minimum score improvement to reset patience\n",
        "            patience: Number of iterations without improvement before stopping\n",
        "            metadata: Dict with 'domain' and 'problem' description\n",
        "            verbose: Print detailed progress\n",
        "\n",
        "        Returns:\n",
        "            result: Dict with:\n",
        "                - X_best: Best feature dataframe\n",
        "                - y: Target variable\n",
        "                - best_score: Best score achieved\n",
        "                - model: Trained model on best features\n",
        "                - feature_documentation: Human-readable explanations\n",
        "                - history: Complete iteration history\n",
        "        \"\"\"\n",
        "        X = df.drop(columns=[target_col])\n",
        "        y = df[target_col]\n",
        "\n",
        "        # Enrich metadata\n",
        "        if metadata is None:\n",
        "            metadata = {}\n",
        "        metadata = self._enrich_metadata(metadata, X, y, target_col)\n",
        "\n",
        "        # Track best results\n",
        "        best_score = 0\n",
        "        best_X = X.copy()\n",
        "        best_iteration_data = None\n",
        "        no_improvement_count = 0\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"üöÄ STARTING EXPLAINABLE FEATURE ENGINEERING PIPELINE\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  Dataset: {X.shape[0]} rows √ó {X.shape[1]} features\")\n",
        "        print(f\"  Target: {target_col} ({self.task_type})\")\n",
        "        print(f\"  Domain: {metadata.get('domain', 'unknown')}\")\n",
        "        print(f\"  Models: {self.evaluator.models}\")\n",
        "        print(f\"  Min Explainability: {self.validator.min_explainability_score}/5\")\n",
        "        print(f\"  Max iterations: {max_iterations} (patience: {patience})\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "        for iteration in range(1, max_iterations + 1):\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"üîÑ ITERATION {iteration}/{max_iterations}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Get feedback from previous iteration\n",
        "            feedback = self._build_feedback(iteration)\n",
        "\n",
        "            # Phase 1: Research (optional)\n",
        "            research_context = None\n",
        "            print(\"\\nüìö Phase 1: Research\")\n",
        "            try:\n",
        "                research_context = self.researcher.search(\n",
        "                target_col,\n",
        "                metadata,\n",
        "                feedback\n",
        "            )\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ö† Research failed: {e}\")\n",
        "                print(\"  Continuing without research insights...\")\n",
        "\n",
        "            # Phase 2: Strategy Design\n",
        "            print(\"\\nüéØ Phase 2: Strategy Design & Validation\")\n",
        "            try:\n",
        "                strategy = self.strategist.design_strategy(\n",
        "                    X,\n",
        "                    target_col,\n",
        "                    feedback,\n",
        "                    research_context\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Strategy design failed: {e}\")\n",
        "                break\n",
        "\n",
        "            # Phase 3: Feature Generation\n",
        "            print(\"\\nüîß Phase 3: Feature Generation\")\n",
        "            try:\n",
        "                generation_result = self.generator.generate(X, strategy)\n",
        "\n",
        "                # Handle both return formats\n",
        "                X_augmented = generation_result['X_new'] if isinstance(generation_result, dict) else generation_result\n",
        "                last_X_augmented = X_augmented\n",
        "                feature_metadata = {}\n",
        "\n",
        "                metadata = self._enrich_metadata(metadata, X_augmented, y, target_col)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Feature generation failed: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                break\n",
        "\n",
        "            # Phase 4: Evaluation\n",
        "            print(\"\\nüìä Phase 4: Evaluation\")\n",
        "            try:\n",
        "                score, top_features, eval_details = self.evaluator.evaluate(\n",
        "                    X_augmented,\n",
        "                    y,\n",
        "                    feature_metadata=feature_metadata,\n",
        "                    return_all_scores=True\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Evaluation failed: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                break\n",
        "\n",
        "            # Record history\n",
        "            iteration_result = {\n",
        "                'iteration': iteration,\n",
        "                'score': score,\n",
        "                'num_features': X_augmented.shape[1],\n",
        "                'top_features': top_features,\n",
        "                'model_scores': eval_details.get('model_scores', {}),\n",
        "                'detailed_metrics': eval_details.get('detailed_metrics', {}),\n",
        "                'strategy': strategy,\n",
        "                'feature_names': list(X_augmented.columns),\n",
        "                'feature_metadata': feature_metadata,\n",
        "                'feature_formulas': self._extract_formulas(strategy),\n",
        "            }\n",
        "            self.history.append(iteration_result)\n",
        "\n",
        "            # Print iteration summary\n",
        "            self._print_iteration_summary(iteration, score, X, X_augmented,\n",
        "                                         top_features, feature_metadata)\n",
        "\n",
        "            # Check improvement\n",
        "            improvement = score - best_score\n",
        "\n",
        "            if improvement > min_improvement:\n",
        "                print(f\"  ‚úÖ Improvement: +{improvement:.4f}\")\n",
        "                best_score = score\n",
        "                best_X = X_augmented.copy()\n",
        "                best_iteration_data = iteration_result\n",
        "                no_improvement_count = 0\n",
        "                X = X_augmented\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è  No significant improvement: +{improvement:.4f}\")\n",
        "                no_improvement_count += 1\n",
        "\n",
        "                if no_improvement_count >= patience:\n",
        "                    print(f\"\\n{'='*70}\")\n",
        "                    print(f\"üõë EARLY STOPPING: No improvement for {patience} iterations\")\n",
        "                    print(f\"{'='*70}\")\n",
        "                    break\n",
        "\n",
        "                X = X_augmented\n",
        "\n",
        "        # Check if we have results\n",
        "        if not self.history:\n",
        "            print(\"\\n‚ö†Ô∏è No iterations completed successfully!\")\n",
        "            return self._create_fallback_result(X, y)\n",
        "\n",
        "        # Get best iteration\n",
        "        best_iteration_data = max(self.history, key=lambda x: x['score'])\n",
        "\n",
        "        # Keep the metadata for docs\n",
        "        self.feature_formulas = best_iteration_data['feature_formulas']\n",
        "        self.feature_metadata = best_iteration_data['feature_metadata']\n",
        "\n",
        "        # Use the best_X you tracked during the loop; fallback to the last augmented X if needed\n",
        "        if best_X is None:\n",
        "            # Fallback: if nothing ever cleared min_improvement, use the final X from the last iteration\n",
        "            # (see the small change below to capture `last_X_augmented` inside the loop)\n",
        "            best_X = last_X_augmented if 'last_X_augmented' in locals() and last_X_augmented is not None else X\n",
        "\n",
        "        self.best_X = best_X\n",
        "        self.best_y = y\n",
        "\n",
        "        # Train final model\n",
        "        print(\"\\nüéì Training final model on best features...\")\n",
        "        self.best_model = self._train_final_model(best_X, y)\n",
        "\n",
        "        # Generate documentation\n",
        "        print(\"\\nüìù Generating feature documentation...\")\n",
        "        feature_docs = self._generate_documentation(best_iteration_data)\n",
        "\n",
        "        # Print final results\n",
        "        self._print_final_results(best_iteration_data)\n",
        "\n",
        "        # Return results\n",
        "        return {\n",
        "            'X_best': best_X,\n",
        "            'y': y,\n",
        "            'best_score': best_score,\n",
        "            'best_iteration': best_iteration_data['iteration'],\n",
        "            'model': self.best_model,\n",
        "            'feature_formulas': self.feature_formulas,\n",
        "            'feature_metadata': self.feature_metadata,\n",
        "            'feature_documentation': feature_docs,\n",
        "            'top_features': best_iteration_data['top_features'],\n",
        "            'history': self.history\n",
        "        }\n",
        "\n",
        "    def _extract_formulas(self, strategy):\n",
        "        \"\"\"Extract formulas from strategy actions\"\"\"\n",
        "        formulas = {}\n",
        "\n",
        "        for action in strategy.get('actions', []):\n",
        "            if action.get('action') == 'create':\n",
        "                name = action.get('name')\n",
        "                formula = action.get('formula')\n",
        "                if name and formula:\n",
        "                    formulas[name] = formula\n",
        "\n",
        "        return formulas\n",
        "\n",
        "    def _train_final_model(self, X, y):\n",
        "        \"\"\"Train a final model on the best feature set\"\"\"\n",
        "        from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "        # Use Random Forest as it's highly explainable\n",
        "        if self.task_type == 'classification':\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=12,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "        else:\n",
        "            model = RandomForestRegressor(\n",
        "                n_estimators=200,\n",
        "                max_depth=12,\n",
        "                min_samples_split=5,\n",
        "                min_samples_leaf=2,\n",
        "                random_state=self.random_state,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "        # Preprocess data\n",
        "        X_cleaned = self.evaluator._clean_data(X)\n",
        "        X_processed = self.evaluator._preprocess_data(X_cleaned)\n",
        "\n",
        "        # Train\n",
        "        model.fit(X_processed, y)\n",
        "\n",
        "        # Get feature importances\n",
        "        importances = dict(zip(X_processed.columns, model.feature_importances_))\n",
        "        top_5 = sorted(importances.items(), key=lambda x: -x[1])[:5]\n",
        "\n",
        "        print(f\"  ‚úì Model trained: {model.__class__.__name__}\")\n",
        "        print(f\"  Top 5 important features:\")\n",
        "        for feat, imp in top_5:\n",
        "            print(f\"    - {feat}: {imp:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _generate_documentation(self, best_iteration_data):\n",
        "        \"\"\"Generate comprehensive human-readable documentation\"\"\"\n",
        "\n",
        "        strategy = best_iteration_data['strategy']\n",
        "        domain = strategy.get('domain', 'Unknown')\n",
        "\n",
        "        # Use strategist to generate documentation\n",
        "        docs = self.strategist.generate_feature_documentation(\n",
        "            strategy.get('actions', []),\n",
        "            domain\n",
        "        )\n",
        "\n",
        "        # Add explainability summary\n",
        "        metadata = best_iteration_data.get('feature_metadata', {})\n",
        "        scores_map = self._safe_scores_from_metadata(metadata)\n",
        "        scores = list(scores_map.values())\n",
        "        if scores:\n",
        "            avg_score = sum(scores) / len(scores)\n",
        "            high_quality = sum(1 for s in scores if s >= 4)\n",
        "            summary = f\"\\n{'='*70}\\nEXPLAINABILITY SUMMARY\\n{'='*70}\\n\"\n",
        "            summary += f\"Average Explainability Score: {avg_score:.2f}/5\\n\"\n",
        "            summary += f\"Highly Explainable Features (‚â•4): {high_quality}/{len(scores)}\\n\"\n",
        "            summary += f\"Total Features: {len(scores)}\\n\"\n",
        "            docs = summary + \"\\n\" + docs\n",
        "\n",
        "        return docs\n",
        "\n",
        "    def _print_iteration_summary(self, iteration, score, X_old, X_new,\n",
        "                             top_features, feature_metadata):\n",
        "      \"\"\"Print detailed iteration summary (safe version)\"\"\"\n",
        "\n",
        "      def _extract_explainability_scores(feature_metadata):\n",
        "        \"\"\"Extract numeric 'score' values robustly.\"\"\"\n",
        "        out = []\n",
        "        if feature_metadata is None:\n",
        "            return out\n",
        "\n",
        "        def _add(v):\n",
        "            try:\n",
        "                val = float(v)\n",
        "                if math.isfinite(val):\n",
        "                    out.append(val)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Dict of dicts or dict of numbers\n",
        "        if isinstance(feature_metadata, dict):\n",
        "            for v in feature_metadata.values():\n",
        "                if isinstance(v, dict):\n",
        "                    s = v.get(\"score\")\n",
        "                    if isinstance(s, (int, float)):\n",
        "                        _add(s)\n",
        "                elif isinstance(v, (int, float)):\n",
        "                    _add(v)\n",
        "        # List/tuple of dicts or numbers\n",
        "        elif isinstance(feature_metadata, (list, tuple)):\n",
        "            for v in feature_metadata:\n",
        "                if isinstance(v, dict):\n",
        "                    s = v.get(\"score\")\n",
        "                    if isinstance(s, (int, float)):\n",
        "                        _add(s)\n",
        "                elif isinstance(v, (int, float)):\n",
        "                    _add(v)\n",
        "        return out\n",
        "\n",
        "      print(f\"\\n{'‚îÄ'*70}\")\n",
        "      print(f\"üìà ITERATION {iteration} SUMMARY\")\n",
        "      print(f\"{'‚îÄ'*70}\")\n",
        "      print(f\"  Score: {score:.4f}\")\n",
        "      print(f\"  Features: {X_old.shape[1]} ‚Üí {X_new.shape[1]}\")\n",
        "\n",
        "      # Explainability stats (robust)\n",
        "      try:\n",
        "          scores = _extract_explainability_scores(feature_metadata)\n",
        "          if scores:\n",
        "              avg_score = sum(scores) / len(scores)\n",
        "              high_quality = sum(1 for s in scores if s >= 4)\n",
        "              medium_quality = sum(1 for s in scores if s == 3)\n",
        "              low_quality = sum(1 for s in scores if s < 3)\n",
        "              print(f\"  Explainability: {avg_score:.1f}/5 avg | \"\n",
        "                    f\"High (‚â•4): {high_quality}, Medium (3): {medium_quality}, Low (<3): {low_quality}\")\n",
        "      except Exception as e:\n",
        "          print(f\"  (Explainability stats unavailable: {e})\")\n",
        "\n",
        "      print(f\"  Top 3 features: {top_features[:3]}\")\n",
        "    def _enrich_metadata(self, metadata, X, y, target_col):\n",
        "        \"\"\"Enrich metadata with dataset statistics\"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        enriched = metadata.copy()\n",
        "\n",
        "        if 'domain' not in enriched:\n",
        "            enriched['domain'] = 'general'\n",
        "\n",
        "        enriched['target'] = target_col\n",
        "        enriched['n_rows'] = len(X)\n",
        "        enriched['n_features'] = len(X.columns)\n",
        "        enriched['headers'] = list(X.columns)\n",
        "        enriched['column_types'] = [str(dtype) for dtype in X.dtypes]\n",
        "        enriched['column_names'] = list(X.columns)\n",
        "\n",
        "        if self.task_type == 'classification':\n",
        "            enriched['n_classes'] = y.nunique()\n",
        "            enriched['class_balance'] = dict(y.value_counts(normalize=True))\n",
        "        else:\n",
        "            enriched['target_mean'] = float(y.mean())\n",
        "            enriched['target_std'] = float(y.std())\n",
        "\n",
        "        enriched['data_stats'] = {\n",
        "            'n_rows': len(X),\n",
        "            'missing_rate': float(X.isna().mean().mean()),\n",
        "            'missing_summary': dict(X.isna().sum()[X.isna().sum() > 0])\n",
        "        }\n",
        "\n",
        "        return enriched\n",
        "\n",
        "    def _build_feedback(self, iteration):\n",
        "        \"\"\"Build feedback from previous iterations\"\"\"\n",
        "        if not self.history:\n",
        "            return None\n",
        "\n",
        "        prev = self.history[-1]\n",
        "\n",
        "        feedback = {\n",
        "            'iteration': iteration - 1,\n",
        "            'best_score': prev['score'],\n",
        "            'top_features': prev['top_features'],\n",
        "            'num_features': prev['num_features'],\n",
        "            'feature_explainability': prev.get('feature_metadata', {})\n",
        "        }\n",
        "\n",
        "        if len(self.history) >= 2:\n",
        "            prev_prev = self.history[-2]\n",
        "            improvement = prev['score'] - prev_prev['score']\n",
        "\n",
        "            if improvement > 0.01:\n",
        "                feedback['what_worked'] = f\"Score improved by {improvement:.4f}\"\n",
        "            elif improvement < -0.01:\n",
        "                feedback['what_failed'] = f\"Score decreased by {abs(improvement):.4f}\"\n",
        "            else:\n",
        "                feedback['what_failed'] = \"Marginal improvement, try different approach\"\n",
        "\n",
        "        if 'model_scores' in prev:\n",
        "            best_model = max(prev['model_scores'].items(), key=lambda x: x[1]['mean'])\n",
        "            feedback['best_model'] = best_model[0]\n",
        "            feedback['best_model_score'] = best_model[1]['mean']\n",
        "\n",
        "        return feedback\n",
        "\n",
        "    def _print_final_results(self, best_iteration_data):\n",
        "        \"\"\"Print comprehensive final results\"\"\"\n",
        "\n",
        "        print(f\"\\n\\n{'='*70}\")\n",
        "        print(f\"üéâ FINAL RESULTS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"  Best iteration: {best_iteration_data['iteration']}/{len(self.history)}\")\n",
        "        print(f\"  Best score: {best_iteration_data['score']:.4f}\")\n",
        "        print(f\"  Features: {best_iteration_data['num_features']}\")\n",
        "\n",
        "        # Explainability summary\n",
        "        metadata = best_iteration_data.get('feature_metadata', {})\n",
        "        scores_map = self._safe_scores_from_metadata(metadata)\n",
        "        if scores_map:\n",
        "            scores = list(scores_map.values())\n",
        "            avg_score = sum(scores) / len(scores)\n",
        "            high_quality = sum(1 for s in scores if s >= 4)\n",
        "            print(f\"  Explainability: {avg_score:.1f}/5 avg, {high_quality}/{len(scores)} highly explainable\")\n",
        "\n",
        "        print(f\"\\n  Top 10 features:\")\n",
        "        for i, feat in enumerate(best_iteration_data['top_features'][:10], 1):\n",
        "            exp_info = \"\"\n",
        "            if feat in scores_map:\n",
        "                exp_info = f\" [Explainability: {scores_map[feat]:.1f}/5]\"\n",
        "            print(f\"    {i:2d}. {feat}{exp_info}\")\n",
        "        if 'model_scores' in best_iteration_data:\n",
        "            print(f\"\\n  Model Performance:\")\n",
        "            for model, scores in best_iteration_data['model_scores'].items():\n",
        "                print(f\"    {model:20s}: {scores['mean']:.4f} (¬±{scores['std']:.4f})\")\n",
        "\n",
        "        print(f\"\\n  Score Progression:\")\n",
        "        for i, result in enumerate(self.history, 1):\n",
        "            improvement = \"\"\n",
        "            if i > 1:\n",
        "                prev_score = self.history[i-2]['score']\n",
        "                diff = result['score'] - prev_score\n",
        "                improvement = f\" ({diff:+.4f})\"\n",
        "            star = \" ‚≠ê\" if result['iteration'] == best_iteration_data['iteration'] else \"\"\n",
        "            print(f\"    Iteration {i:2d}: {result['score']:.4f}{improvement}{star}\")\n",
        "\n",
        "        print(f\"{'='*70}\\n\")\n",
        "\n",
        "    def _create_fallback_result(self, X, y):\n",
        "        \"\"\"Create fallback result when no iterations complete\"\"\"\n",
        "        print(\"\\n‚ö†Ô∏è No iterations completed successfully!\")\n",
        "\n",
        "        return {\n",
        "            'X_best': X.copy(),\n",
        "            'y': y,\n",
        "            'best_score': 0.0,\n",
        "            'best_iteration': 0,\n",
        "            'model': None,\n",
        "            'feature_formulas': {},\n",
        "            'feature_metadata': {},\n",
        "            'feature_documentation': \"No documentation available\",\n",
        "            'top_features': [],\n",
        "            'history': []\n",
        "        }\n",
        "\n",
        "    def save_results(self, result, base_filename='pipeline_results'):\n",
        "        \"\"\"\n",
        "        Save all results to files\n",
        "\n",
        "        Args:\n",
        "            result: Result dict from run()\n",
        "            base_filename: Base name for output files\n",
        "        \"\"\"\n",
        "        import pickle\n",
        "        import pandas as pd\n",
        "        import json\n",
        "\n",
        "        print(f\"\\nüíæ Saving results...\")\n",
        "\n",
        "        # 1. Save best dataset (X + y)\n",
        "        df_best = result['X_best'].copy()\n",
        "        df_best['target'] = result['y']\n",
        "        csv_path = f\"{base_filename}_data.csv\"\n",
        "        df_best.to_csv(csv_path, index=False)\n",
        "        print(f\"  ‚úì Best dataset saved to {csv_path}\")\n",
        "\n",
        "        # 2. Save trained model\n",
        "        if result['model']:\n",
        "            model_path = f\"{base_filename}_model.pkl\"\n",
        "            with open(model_path, 'wb') as f:\n",
        "                pickle.dump(result['model'], f)\n",
        "            print(f\"  ‚úì Trained model saved to {model_path}\")\n",
        "\n",
        "        # 3. Save feature documentation\n",
        "        if result['feature_documentation']:\n",
        "            doc_path = f\"{base_filename}_features.txt\"\n",
        "            with open(doc_path, 'w') as f:\n",
        "                f.write(result['feature_documentation'])\n",
        "            print(f\"  ‚úì Feature documentation saved to {doc_path}\")\n",
        "\n",
        "        # 4. Save feature formulas and metadata\n",
        "        metadata_path = f\"{base_filename}_metadata.json\"\n",
        "        metadata_export = {\n",
        "            'feature_formulas': result['feature_formulas'],\n",
        "            'feature_metadata': result['feature_metadata'],\n",
        "            'best_score': result['best_score'],\n",
        "            'best_iteration': result['best_iteration'],\n",
        "            'top_features': result['top_features'],\n",
        "            'num_features': len(result['X_best'].columns),\n",
        "            'explainability_summary': self._get_explainability_summary(result)\n",
        "        }\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata_export, f, indent=2)\n",
        "        print(f\"  ‚úì Feature metadata saved to {metadata_path}\")\n",
        "\n",
        "        # 5. Save complete history\n",
        "        history_path = f\"{base_filename}_history.json\"\n",
        "        # Convert history to JSON-serializable format\n",
        "        history_export = []\n",
        "        for h in result['history']:\n",
        "            h_copy = h.copy()\n",
        "            # Remove non-serializable items\n",
        "            if 'detailed_metrics' in h_copy:\n",
        "                del h_copy['detailed_metrics']\n",
        "            history_export.append(h_copy)\n",
        "\n",
        "        with open(history_path, 'w') as f:\n",
        "            json.dump(history_export, f, indent=2)\n",
        "        print(f\"  ‚úì Complete history saved to {history_path}\")\n",
        "\n",
        "        print(f\"\\n‚úÖ All results saved with base name: {base_filename}\")\n",
        "\n",
        "        return {\n",
        "            'data': csv_path,\n",
        "            'model': model_path if result['model'] else None,\n",
        "            'documentation': doc_path,\n",
        "            'metadata': metadata_path,\n",
        "            'history': history_path\n",
        "        }\n",
        "\n",
        "    def _get_explainability_summary(self, result):\n",
        "        \"\"\"Get explainability summary statistics\"\"\"\n",
        "        metadata = result.get('feature_metadata', {})\n",
        "\n",
        "        if not metadata:\n",
        "            return {}\n",
        "\n",
        "        scores = [m['score'] for m in metadata.values() if 'score' in m]\n",
        "\n",
        "        if not scores:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'average_score': sum(scores) / len(scores),\n",
        "            'high_quality_count': sum(1 for s in scores if s >= 4),\n",
        "            'medium_quality_count': sum(1 for s in scores if 3 <= s < 4),\n",
        "            'low_quality_count': sum(1 for s in scores if s < 3),\n",
        "            'total_features': len(scores)\n",
        "        }\n",
        "\n",
        "    def plot_progression(self):\n",
        "        \"\"\"Plot score progression over iterations\"\"\"\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "\n",
        "            iterations = [h['iteration'] for h in self.history]\n",
        "            scores = [h['score'] for h in self.history]\n",
        "\n",
        "            plt.figure(figsize=(12, 6))\n",
        "\n",
        "            # Plot score progression\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(iterations, scores, marker='o', linewidth=2, markersize=8)\n",
        "            plt.xlabel('Iteration', fontsize=12)\n",
        "            plt.ylabel('Score', fontsize=12)\n",
        "            plt.title('Score Progression', fontsize=14, fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot explainability progression\n",
        "            plt.subplot(1, 2, 2)\n",
        "            explainability_avgs = []\n",
        "            for h in self.history:\n",
        "                metadata = h.get('feature_metadata', {})\n",
        "                if metadata:\n",
        "                    scores_list = [m['score'] for m in metadata.values() if 'score' in m]\n",
        "                    if scores_list:\n",
        "                        explainability_avgs.append(sum(scores_list) / len(scores_list))\n",
        "                    else:\n",
        "                        explainability_avgs.append(0)\n",
        "                else:\n",
        "                    explainability_avgs.append(0)\n",
        "\n",
        "            if any(explainability_avgs):\n",
        "                plt.plot(iterations, explainability_avgs, marker='s',\n",
        "                        linewidth=2, markersize=8, color='green')\n",
        "                plt.xlabel('Iteration', fontsize=12)\n",
        "                plt.ylabel('Avg Explainability Score', fontsize=12)\n",
        "                plt.title('Explainability Progression', fontsize=14, fontweight='bold')\n",
        "                plt.ylim(0, 5)\n",
        "                plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"matplotlib not installed. Install with: pip install matplotlib\")"
      ],
      "metadata": {
        "id": "t2l_b4xuZ0a5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    from google.colab import userdata\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"üè• STROKE PREDICTION - FEATURE ENGINEERING PIPELINE\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # ==================== 1. LOAD DATA ====================\n",
        "    print(\"üìÅ Loading dataset...\")\n",
        "    df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')\n",
        "\n",
        "    print(f\"  Shape: {df.shape}\")\n",
        "    print(f\"  Target distribution:\\n{df['stroke'].value_counts()}\")\n",
        "    print(f\"  Class imbalance: {(df['stroke']==0).sum()}/{(df['stroke']==1).sum()} (no stroke/stroke)\")\n",
        "\n",
        "    # ==================== 2. PREPROCESS DATA ====================\n",
        "    print(\"\\nüîß Preprocessing data...\")\n",
        "\n",
        "    # Drop ID column\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "        print(\"  ‚úì Dropped 'id' column\")\n",
        "\n",
        "    # Handle missing BMI values\n",
        "    if df['bmi'].isnull().any():\n",
        "        n_missing = df['bmi'].isnull().sum()\n",
        "        df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n",
        "        print(f\"  ‚úì Imputed {n_missing} missing BMI values with median\")\n",
        "\n",
        "    # Encode categorical variables\n",
        "    categorical_cols = ['gender', 'ever_married', 'work_type',\n",
        "                       'Residence_type', 'smoking_status']\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "    print(f\"  ‚úì Encoded {len(categorical_cols)} categorical columns\")\n",
        "    print(f\"  ‚úì Final shape: {df.shape}\")\n",
        "\n",
        "    # ==================== 3. SETUP METADATA ====================\n",
        "    metadata = {\n",
        "        'domain': 'healthcare - cardiovascular/neurology',\n",
        "        'problem': 'stroke risk prediction',\n",
        "        'description': '''Predicting stroke occurrence based on patient demographics,\n",
        "        health metrics (hypertension, heart disease, BMI, glucose), and lifestyle factors\n",
        "        (smoking, work type). Dataset is highly imbalanced (~5% stroke rate).'''\n",
        "    }\n",
        "\n",
        "    # ==================== 4. INITIALIZE PIPELINE ====================\n",
        "    print(\"\\nüöÄ Initializing pipeline...\")\n",
        "\n",
        "    pipeline = FeatureEngineeringPipeline(\n",
        "        api_key=userdata.get('openaiapi'),\n",
        "        task_type='classification',\n",
        "        models=['random_forest', 'xgboost', 'gradient_boosting'],\n",
        "        preprocessing='robust',  # Good for medical data with outliers\n",
        "        n_cv_folds=5,\n",
        "        min_explainability_score=3  # Require explainable features\n",
        "    )\n",
        "\n",
        "    # ==================== 5. RUN PIPELINE ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚ñ∂Ô∏è  RUNNING FEATURE ENGINEERING PIPELINE\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    result = pipeline.run(\n",
        "        df=df,\n",
        "        target_col='stroke',\n",
        "        max_iterations=15,\n",
        "        min_improvement=0.005,  # Small improvements matter for imbalanced data\n",
        "        patience=3,\n",
        "        metadata=metadata,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # ==================== 6. SAVE RESULTS ====================\n",
        "    print(\"\\nüíæ Saving results...\")\n",
        "\n",
        "    saved_files = pipeline.save_results(\n",
        "        result,\n",
        "        base_filename='stroke_prediction'\n",
        "    )\n",
        "\n",
        "    # ==================== 7. PRINT SUMMARY ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä FINAL SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\nüéØ Performance:\")\n",
        "    print(f\"  Best Score: {result['best_score']:.4f}\")\n",
        "    print(f\"  Best Iteration: {result['best_iteration']}\")\n",
        "    print(f\"  Input Features: {df.shape[1]-1}\")\n",
        "    print(f\"  Output Features: {len(result['X_best'].columns)}\")\n",
        "\n",
        "    # Model breakdown\n",
        "    if result.get('model_scores'):\n",
        "        print(f\"\\nü§ñ Model Performance:\")\n",
        "        for model, scores in result['model_scores'].items():\n",
        "            print(f\"  {model:20s}: {scores['mean']:.4f} ¬± {scores['std']:.4f}\")\n",
        "\n",
        "    # Top features\n",
        "    print(f\"\\n‚≠ê Top 10 Features:\")\n",
        "    for i, feat in enumerate(result['top_features'][:10], 1):\n",
        "        exp_info = \"\"\n",
        "        if feat in result['feature_metadata'] and 'score' in result['feature_metadata'][feat]:\n",
        "            exp_score = result['feature_metadata'][feat]['score']\n",
        "            exp_info = f\" [Explainability: {exp_score}/5]\"\n",
        "        print(f\"  {i:2d}. {feat}{exp_info}\")\n",
        "\n",
        "    # Explainability summary\n",
        "    metadata_scores = [m['score'] for m in result['feature_metadata'].values() if 'score' in m]\n",
        "    if metadata_scores:\n",
        "        avg_exp = sum(metadata_scores) / len(metadata_scores)\n",
        "        high_exp = sum(1 for s in metadata_scores if s >= 4)\n",
        "        print(f\"\\nüîç Explainability:\")\n",
        "        print(f\"  Average Score: {avg_exp:.1f}/5\")\n",
        "        print(f\"  Highly Explainable (‚â•4): {high_exp}/{len(metadata_scores)}\")\n",
        "\n",
        "    # Progression\n",
        "    print(f\"\\nüìà Score Progression:\")\n",
        "    for i, h in enumerate(result['history'], 1):\n",
        "        change = \"\"\n",
        "        if i > 1:\n",
        "            diff = h['score'] - result['history'][i-2]['score']\n",
        "            change = f\" ({diff:+.4f})\"\n",
        "        star = \" ‚≠ê\" if h['iteration'] == result['best_iteration'] else \"\"\n",
        "        print(f\"  Iteration {i:2d}: {h['score']:.4f}{change}{star}\")\n",
        "\n",
        "    # Saved files\n",
        "    print(f\"\\nüìÇ Saved Files:\")\n",
        "    for file_type, filepath in saved_files.items():\n",
        "        if filepath:\n",
        "            print(f\"  ‚Ä¢ {file_type}: {filepath}\")\n",
        "\n",
        "    # ==================== 8. DOMAIN-SPECIFIC INSIGHTS ====================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üè• STROKE-SPECIFIC INSIGHTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Identify new features created\n",
        "    original_features = set(df.columns) - {'stroke'}\n",
        "    new_features = [f for f in result['X_best'].columns if f not in original_features]\n",
        "\n",
        "    print(f\"\\n‚ú® Created {len(new_features)} new features\")\n",
        "\n",
        "    # Check for clinically relevant features in top 10\n",
        "    clinical_keywords = ['age', 'hypertension', 'heart', 'glucose', 'bmi',\n",
        "                        'smoking', 'cardiovascular', 'risk']\n",
        "\n",
        "    clinical_features = [\n",
        "        f for f in result['top_features'][:15]\n",
        "        if any(kw in f.lower() for kw in clinical_keywords)\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n‚öïÔ∏è  Clinical Risk Features in Top 15: {len(clinical_features)}\")\n",
        "    if clinical_features:\n",
        "        for feat in clinical_features[:8]:\n",
        "            # Try to get explanation if available\n",
        "            if feat in result['feature_metadata']:\n",
        "                meta = result['feature_metadata'][feat]\n",
        "                rationale = meta.get('rationale', 'N/A')\n",
        "                if rationale != 'N/A' and len(rationale) > 0:\n",
        "                    print(f\"  ‚Ä¢ {feat}\")\n",
        "                    print(f\"    ‚îî‚îÄ {rationale[:80]}...\")\n",
        "                else:\n",
        "                    print(f\"  ‚Ä¢ {feat}\")\n",
        "            else:\n",
        "                print(f\"  ‚Ä¢ {feat}\")\n",
        "\n",
        "    # Check for interaction features\n",
        "    interaction_features = [\n",
        "        f for f in new_features\n",
        "        if any(op in f for op in ['*', '/', '_x_', '_ratio', '_interaction'])\n",
        "    ]\n",
        "\n",
        "    if interaction_features:\n",
        "        print(f\"\\nüîó Interaction Features: {len(interaction_features)}\")\n",
        "        for feat in interaction_features[:5]:\n",
        "            print(f\"  ‚Ä¢ {feat}\")\n",
        "\n",
        "    # ==================== 9. OPTIONAL VISUALIZATION ====================\n",
        "    try:\n",
        "        print(\"\\nüìä Generating progression plots...\")\n",
        "        pipeline.plot_progression()\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Could not generate plots: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nNext steps:\")\n",
        "    print(f\"  1. Review feature documentation: {saved_files.get('documentation')}\")\n",
        "    print(f\"  2. Examine feature formulas: {saved_files.get('metadata')}\")\n",
        "    print(f\"  3. Use trained model: result['model'] for predictions\")\n",
        "    print(f\"  4. Load best data: pd.read_csv('{saved_files.get('data')}')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lCvAVxhviMig",
        "outputId": "43cd9d7f-6f80-4d58-c3dd-e8fb61d800b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üè• STROKE PREDICTION - FEATURE ENGINEERING PIPELINE\n",
            "======================================================================\n",
            "\n",
            "üìÅ Loading dataset...\n",
            "  Shape: (5110, 12)\n",
            "  Target distribution:\n",
            "stroke\n",
            "0    4861\n",
            "1     249\n",
            "Name: count, dtype: int64\n",
            "  Class imbalance: 4861/249 (no stroke/stroke)\n",
            "\n",
            "üîß Preprocessing data...\n",
            "  ‚úì Dropped 'id' column\n",
            "  ‚úì Imputed 201 missing BMI values with median\n",
            "  ‚úì Encoded 5 categorical columns\n",
            "  ‚úì Final shape: (5110, 11)\n",
            "\n",
            "üöÄ Initializing pipeline...\n",
            "\n",
            "======================================================================\n",
            "‚ñ∂Ô∏è  RUNNING FEATURE ENGINEERING PIPELINE\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üöÄ STARTING EXPLAINABLE FEATURE ENGINEERING PIPELINE\n",
            "======================================================================\n",
            "  Dataset: 5110 rows √ó 10 features\n",
            "  Target: stroke (classification)\n",
            "  Domain: healthcare - cardiovascular/neurology\n",
            "  Models: ['random_forest', 'xgboost', 'gradient_boosting']\n",
            "  Min Explainability: 3/5\n",
            "  Max iterations: 15 (patience: 3)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "üîÑ ITERATION 1/15\n",
            "======================================================================\n",
            "\n",
            "üìö Phase 1: Research\n",
            "  üîç Performing physiologically-focused research with OpenAI...\n",
            "  ‚úì Extracted 3 physiologically meaningful features\n",
            "  üí° The model focuses on well-established clinical risk factors for stroke, such as age, hypertension, B...\n",
            "\n",
            "üéØ Phase 2: Strategy Design & Validation\n",
            "\n",
            "  üìã Agent Strategy:\n",
            "    Domain: healthcare - neurology/cardiology\n",
            "    Reasoning: The feature engineering strategy focuses on deriving clinically relevant and physiologically explain...\n",
            "    Explainability: Avg 3.8/5\n",
            "    Quality: 2 high (4-5), 2 medium (3), 0 low (<3)\n",
            "  üîç Validating strategy (technical + explainability)...\n",
            "  ‚úì Keep: age (Explainability: 5)\n",
            "  ‚úì Keep: hypertension (Explainability: 5)\n",
            "  ‚úì Keep: avg_glucose_level (Explainability: 5)\n",
            "  ‚úì Keep: bmi (Explainability: 5)\n",
            "  ‚úì Create: age_hypertension_interaction (Explainability: 5/5, Clinical: HIGH)\n",
            "  ‚úì Create: bmi_smoking_interaction (Explainability: 4/5, Clinical: HIGH)\n",
            "  ‚úì Create: log_avg_glucose_level (Explainability: 3/5, Clinical: MEDIUM)\n",
            "  ‚úì Create: age_squared (Explainability: 3/5, Clinical: MEDIUM)\n",
            "  ‚úì Validated 8 actions: Keep=4, Create=4, Remove=0\n",
            "  üìä Explainability: Avg 3.8/5 | High (‚â•4): 2, Medium (3): 2, Low (<3): 0\n",
            "\n",
            "üîß Phase 3: Feature Generation\n",
            "\n",
            "  üîß Executing feature actions:\n",
            "  ‚úì Keeping: age\n",
            "  ‚úì Keeping: hypertension\n",
            "  ‚úì Keeping: avg_glucose_level\n",
            "  ‚úì Keeping: bmi\n",
            "  ‚úì Created: age_hypertension_interaction (Explainability: 5/5)\n",
            "  ‚úì Created: bmi_smoking_interaction (Explainability: 4/5)\n",
            "  ‚úì Created: log_avg_glucose_level (Explainability: 3/5)\n",
            "  ‚úì Created: age_squared (Explainability: 3/5)\n",
            "\n",
            "  üîç Final data quality check:\n",
            "    ‚úì Passed quality checks\n",
            "\n",
            "  üìä Feature Generation Summary:\n",
            "     Input features:   10\n",
            "     Output features:  8\n",
            "     Kept:             4\n",
            "     Created:          4\n",
            "     Removed:          0\n",
            "     Failed:           0\n",
            "\n",
            "üìä Phase 4: Evaluation\n",
            "  üßπ Cleaning data...\n",
            "    ‚úì Cleaned: 8 ‚Üí 8 features\n",
            "  ‚öôÔ∏è Preprocessing data...\n",
            "    ‚úì Applied robust scaling\n",
            "\n",
            "  üìä Evaluating with 3 explainable models (5-fold CV):\n",
            "    random_forest       : 0.1705 (¬±0.0241) [Explainable ‚úì]\n",
            "    xgboost             : 0.1666 (¬±0.0261) [Explainable ‚úì]\n",
            "    gradient_boosting   : 0.1579 (¬±0.0260) [Explainable ‚úì]\n",
            "\n",
            "  ‚úì Average average_precision: 0.1650\n",
            "\n",
            "  üìà Detailed Performance Metrics (averaged across models):\n",
            "    Accuracy:  0.9129 (¬±0.0273)\n",
            "    Precision: 0.5768 (¬±0.0065) [True Pos / (True Pos + False Pos)]\n",
            "    Recall:    0.5845 (¬±0.0520) [True Pos / (True Pos + False Neg)]\n",
            "    F1 Score:  0.5635 (¬±0.0294) [Harmonic mean of precision & recall]\n",
            "    ROC-AUC:   0.8094 (¬±0.0049)\n",
            "  ‚úì Top features: ['age_squared', 'age', 'bmi', 'avg_glucose_level', 'log_avg_glucose_level']\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìà ITERATION 1 SUMMARY\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  Score: 0.1650\n",
            "  Features: 10 ‚Üí 8\n",
            "  Top 3 features: ['age_squared', 'age', 'bmi']\n",
            "  ‚úÖ Improvement: +0.1650\n",
            "\n",
            "======================================================================\n",
            "üîÑ ITERATION 2/15\n",
            "======================================================================\n",
            "\n",
            "üìö Phase 1: Research\n",
            "  üîç Performing physiologically-focused research with OpenAI...\n",
            "  ‚úì Extracted 4 physiologically meaningful features\n",
            "  üí° The model should focus on age, glucose levels, BMI, and hypertension as primary features due to thei...\n",
            "\n",
            "üéØ Phase 2: Strategy Design & Validation\n",
            "\n",
            "  üìã Agent Strategy:\n",
            "    Domain: healthcare - neurology\n",
            "    Reasoning: The strategy focuses on maintaining features with strong clinical relevance and physiological explan...\n",
            "    Explainability: Avg 4.0/5\n",
            "    Quality: 3 high (4-5), 0 medium (3), 0 low (<3)\n",
            "  üîç Validating strategy (technical + explainability)...\n",
            "  ‚ö† Auto-keeping top feature: age_squared\n",
            "  ‚úì Keep: age (Explainability: 5)\n",
            "  ‚úì Keep: avg_glucose_level (Explainability: 5)\n",
            "  ‚úì Keep: bmi (Explainability: 5)\n",
            "  ‚úì Keep: log_avg_glucose_level (Explainability: 3)\n",
            "  ‚úì Create: bmi_age_interaction (Explainability: 4/5, Clinical: HIGH)\n",
            "  ‚úì Create: hypertension_age_interaction (Explainability: 4/5, Clinical: HIGH)\n",
            "  ‚úì Create: bmi_glucose_interaction (Explainability: 4/5, Clinical: HIGH)\n",
            "  ‚úì Remove: bmi_smoking_interaction\n",
            "  ‚úì Remove: age_hypertension_interaction\n",
            "  ‚úì Validated 10 actions: Keep=5, Create=3, Remove=2\n",
            "  üìä Explainability: Avg 4.0/5 | High (‚â•4): 3, Medium (3): 0, Low (<3): 0\n",
            "\n",
            "üîß Phase 3: Feature Generation\n",
            "\n",
            "  üîß Executing feature actions:\n",
            "  ‚úì Keeping: age_squared\n",
            "  ‚úì Keeping: age\n",
            "  ‚úì Keeping: avg_glucose_level\n",
            "  ‚úì Keeping: bmi\n",
            "  ‚úì Keeping: log_avg_glucose_level\n",
            "  ‚úì Created: bmi_age_interaction (Explainability: 4/5)\n",
            "  ‚úì Created: hypertension_age_interaction (Explainability: 4/5)\n",
            "  ‚úì Created: bmi_glucose_interaction (Explainability: 4/5)\n",
            "  ‚úì Removed: bmi_smoking_interaction\n",
            "  ‚úì Removed: age_hypertension_interaction\n",
            "\n",
            "  üîç Final data quality check:\n",
            "    ‚úì Passed quality checks\n",
            "\n",
            "  üìä Feature Generation Summary:\n",
            "     Input features:   8\n",
            "     Output features:  8\n",
            "     Kept:             5\n",
            "     Created:          3\n",
            "     Removed:          2\n",
            "     Failed:           0\n",
            "\n",
            "üìä Phase 4: Evaluation\n",
            "  üßπ Cleaning data...\n",
            "    ‚úì Cleaned: 8 ‚Üí 8 features\n",
            "  ‚öôÔ∏è Preprocessing data...\n",
            "    ‚úì Applied robust scaling\n",
            "\n",
            "  üìä Evaluating with 3 explainable models (5-fold CV):\n",
            "    random_forest       : 0.1615 (¬±0.0217) [Explainable ‚úì]\n",
            "    xgboost             : 0.1486 (¬±0.0280) [Explainable ‚úì]\n",
            "    gradient_boosting   : 0.1508 (¬±0.0117) [Explainable ‚úì]\n",
            "\n",
            "  ‚úì Average average_precision: 0.1536\n",
            "\n",
            "  üìà Detailed Performance Metrics (averaged across models):\n",
            "    Accuracy:  0.9145 (¬±0.0235)\n",
            "    Precision: 0.5640 (¬±0.0042) [True Pos / (True Pos + False Pos)]\n",
            "    Recall:    0.5676 (¬±0.0397) [True Pos / (True Pos + False Neg)]\n",
            "    F1 Score:  0.5537 (¬±0.0275) [Harmonic mean of precision & recall]\n",
            "    ROC-AUC:   0.8118 (¬±0.0103)\n",
            "  ‚úì Top features: ['bmi_age_interaction', 'age', 'age_squared', 'log_avg_glucose_level', 'avg_glucose_level']\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìà ITERATION 2 SUMMARY\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  Score: 0.1536\n",
            "  Features: 8 ‚Üí 8\n",
            "  Top 3 features: ['bmi_age_interaction', 'age', 'age_squared']\n",
            "  ‚ö†Ô∏è  No significant improvement: +-0.0114\n",
            "\n",
            "======================================================================\n",
            "üîÑ ITERATION 3/15\n",
            "======================================================================\n",
            "\n",
            "üìö Phase 1: Research\n",
            "  üîç Performing physiologically-focused research with OpenAI...\n",
            "  ‚úì Extracted 4 physiologically meaningful features\n",
            "  üí° Age and average glucose level are primary features for stroke risk prediction due to their strong cl...\n",
            "\n",
            "üéØ Phase 2: Strategy Design & Validation\n",
            "\n",
            "  üìã Agent Strategy:\n",
            "    Domain: healthcare - neurology\n",
            "    Reasoning: The focus is on creating features that are clinically relevant and physiologically explainable in th...\n",
            "    Explainability: Avg 3.7/5\n",
            "    Quality: 2 high (4-5), 1 medium (3), 0 low (<3)\n",
            "  üîç Validating strategy (technical + explainability)...\n",
            "  ‚ö† Auto-keeping top feature: age_squared\n",
            "  ‚úì Keep: age (Explainability: 5)\n",
            "  ‚úì Keep: avg_glucose_level (Explainability: 5)\n",
            "  ‚úì Keep: bmi_age_interaction (Explainability: 4)\n",
            "  ‚úì Keep: log_avg_glucose_level (Explainability: 3)\n",
            "  ‚úì Create: bmi_squared (Explainability: 3/5, Clinical: MEDIUM)\n",
            "  ‚úì Create: age_bmi_ratio (Explainability: 4/5, Clinical: MEDIUM)\n",
            "  ‚úì Create: glucose_bmi_interaction (Explainability: 4/5, Clinical: HIGH)\n",
            "  ‚úì Remove: hypertension_age_interaction\n",
            "  ‚úì Validated 9 actions: Keep=5, Create=3, Remove=1\n",
            "  üìä Explainability: Avg 3.7/5 | High (‚â•4): 2, Medium (3): 1, Low (<3): 0\n",
            "\n",
            "üîß Phase 3: Feature Generation\n",
            "\n",
            "  üîß Executing feature actions:\n",
            "  ‚úì Keeping: age_squared\n",
            "  ‚úì Keeping: age\n",
            "  ‚úì Keeping: avg_glucose_level\n",
            "  ‚úì Keeping: bmi_age_interaction\n",
            "  ‚úì Keeping: log_avg_glucose_level\n",
            "  ‚úì Created: bmi_squared (Explainability: 3/5)\n",
            "  ‚úì Created: age_bmi_ratio (Explainability: 4/5)\n",
            "  ‚úì Created: glucose_bmi_interaction (Explainability: 4/5)\n",
            "  ‚úì Removed: hypertension_age_interaction\n",
            "\n",
            "  üîç Final data quality check:\n",
            "    ‚úì Passed quality checks\n",
            "\n",
            "  üìä Feature Generation Summary:\n",
            "     Input features:   8\n",
            "     Output features:  8\n",
            "     Kept:             5\n",
            "     Created:          3\n",
            "     Removed:          1\n",
            "     Failed:           0\n",
            "\n",
            "üìä Phase 4: Evaluation\n",
            "  üßπ Cleaning data...\n",
            "    ‚úì Cleaned: 8 ‚Üí 8 features\n",
            "  ‚öôÔ∏è Preprocessing data...\n",
            "    ‚úì Applied robust scaling\n",
            "\n",
            "  üìä Evaluating with 3 explainable models (5-fold CV):\n",
            "    random_forest       : 0.1595 (¬±0.0190) [Explainable ‚úì]\n",
            "    xgboost             : 0.1461 (¬±0.0121) [Explainable ‚úì]\n",
            "    gradient_boosting   : 0.1484 (¬±0.0071) [Explainable ‚úì]\n",
            "\n",
            "  ‚úì Average average_precision: 0.1513\n",
            "\n",
            "  üìà Detailed Performance Metrics (averaged across models):\n",
            "    Accuracy:  0.9139 (¬±0.0234)\n",
            "    Precision: 0.5497 (¬±0.0245) [True Pos / (True Pos + False Pos)]\n",
            "    Recall:    0.5704 (¬±0.0485) [True Pos / (True Pos + False Neg)]\n",
            "    F1 Score:  0.5518 (¬±0.0360) [Harmonic mean of precision & recall]\n",
            "    ROC-AUC:   0.8115 (¬±0.0087)\n",
            "  ‚úì Top features: ['bmi_age_interaction', 'age', 'age_squared', 'age_bmi_ratio', 'log_avg_glucose_level']\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìà ITERATION 3 SUMMARY\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  Score: 0.1513\n",
            "  Features: 8 ‚Üí 8\n",
            "  Top 3 features: ['bmi_age_interaction', 'age', 'age_squared']\n",
            "  ‚ö†Ô∏è  No significant improvement: +-0.0136\n",
            "\n",
            "======================================================================\n",
            "üîÑ ITERATION 4/15\n",
            "======================================================================\n",
            "\n",
            "üìö Phase 1: Research\n",
            "  üîç Performing physiologically-focused research with OpenAI...\n",
            "  ‚úì Extracted 4 physiologically meaningful features\n",
            "  üí° Age and average glucose level are critical features for stroke risk prediction. Interactions between...\n",
            "\n",
            "üéØ Phase 2: Strategy Design & Validation\n",
            "\n",
            "  üìã Agent Strategy:\n",
            "    Domain: healthcare - neurology\n",
            "    Reasoning: The strategy focuses on enhancing the interpretability of features related to stroke risk by leverag...\n",
            "    Explainability: Avg 3.5/5\n",
            "    Quality: 1 high (4-5), 1 medium (3), 0 low (<3)\n",
            "  üîç Validating strategy (technical + explainability)...\n",
            "  ‚úì Keep: bmi_age_interaction (Explainability: 3)\n",
            "  ‚úì Keep: age (Explainability: 5)\n",
            "  ‚úì Keep: age_squared (Explainability: 2)\n",
            "  ‚úì Keep: age_bmi_ratio (Explainability: 3)\n",
            "  ‚úì Keep: log_avg_glucose_level (Explainability: 3)\n",
            "  ‚úì Create: age_glucose_interaction (Explainability: 4/5, Clinical: HIGH)\n",
            "  ‚úì Create: bmi_log_glucose_interaction (Explainability: 3/5, Clinical: MEDIUM)\n",
            "  ‚úì Remove: glucose_bmi_interaction\n",
            "  ‚úì Validated 8 actions: Keep=5, Create=2, Remove=1\n",
            "  üìä Explainability: Avg 3.5/5 | High (‚â•4): 1, Medium (3): 1, Low (<3): 0\n",
            "\n",
            "üîß Phase 3: Feature Generation\n",
            "\n",
            "  üîß Executing feature actions:\n",
            "  ‚úì Keeping: bmi_age_interaction\n",
            "  ‚úì Keeping: age\n",
            "  ‚úì Keeping: age_squared\n",
            "  ‚úì Keeping: age_bmi_ratio\n",
            "  ‚úì Keeping: log_avg_glucose_level\n",
            "  ‚úì Created: age_glucose_interaction (Explainability: 4/5)\n",
            "  ‚ùå Failed to create 'bmi_log_glucose_interaction': 'bmi_age_interaction'\n",
            "  ‚úì Removed: glucose_bmi_interaction\n",
            "\n",
            "  üîç Final data quality check:\n",
            "    ‚úì Passed quality checks\n",
            "\n",
            "  üìä Feature Generation Summary:\n",
            "     Input features:   8\n",
            "     Output features:  6\n",
            "     Kept:             5\n",
            "     Created:          1\n",
            "     Removed:          1\n",
            "     Failed:           1\n",
            "\n",
            "  ‚ö† Failed features:\n",
            "     - bmi_log_glucose_interaction: 'bmi_age_interaction'\n",
            "\n",
            "üìä Phase 4: Evaluation\n",
            "  üßπ Cleaning data...\n",
            "    ‚úì Cleaned: 6 ‚Üí 6 features\n",
            "  ‚öôÔ∏è Preprocessing data...\n",
            "    ‚úì Applied robust scaling\n",
            "\n",
            "  üìä Evaluating with 3 explainable models (5-fold CV):\n",
            "    random_forest       : 0.1565 (¬±0.0170) [Explainable ‚úì]\n",
            "    xgboost             : 0.1385 (¬±0.0156) [Explainable ‚úì]\n",
            "    gradient_boosting   : 0.1460 (¬±0.0129) [Explainable ‚úì]\n",
            "\n",
            "  ‚úì Average average_precision: 0.1470\n",
            "\n",
            "  üìà Detailed Performance Metrics (averaged across models):\n",
            "    Accuracy:  0.9079 (¬±0.0302)\n",
            "    Precision: 0.5507 (¬±0.0173) [True Pos / (True Pos + False Pos)]\n",
            "    Recall:    0.5680 (¬±0.0445) [True Pos / (True Pos + False Neg)]\n",
            "    F1 Score:  0.5474 (¬±0.0330) [Harmonic mean of precision & recall]\n",
            "    ROC-AUC:   0.8070 (¬±0.0099)\n",
            "  ‚úì Top features: ['bmi_age_interaction', 'age', 'age_glucose_interaction', 'age_squared', 'log_avg_glucose_level']\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "üìà ITERATION 4 SUMMARY\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "  Score: 0.1470\n",
            "  Features: 8 ‚Üí 6\n",
            "  Top 3 features: ['bmi_age_interaction', 'age', 'age_glucose_interaction']\n",
            "  ‚ö†Ô∏è  No significant improvement: +-0.0180\n",
            "\n",
            "======================================================================\n",
            "üõë EARLY STOPPING: No improvement for 3 iterations\n",
            "======================================================================\n",
            "\n",
            "üéì Training final model on best features...\n",
            "  üßπ Cleaning data...\n",
            "    ‚úì Cleaned: 8 ‚Üí 8 features\n",
            "  ‚öôÔ∏è Preprocessing data...\n",
            "    ‚úì Applied robust scaling\n",
            "  ‚úì Model trained: RandomForestClassifier\n",
            "  Top 5 important features:\n",
            "    - avg_glucose_level: 0.2013\n",
            "    - log_avg_glucose_level: 0.1954\n",
            "    - bmi: 0.1767\n",
            "    - age_squared: 0.1484\n",
            "    - age: 0.1453\n",
            "\n",
            "üìù Generating feature documentation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FeatureEngineeringPipeline' object has no attribute '_safe_scores_from_metadata'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2597186743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     result = pipeline.run(\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stroke'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2955397916.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, df, target_col, max_iterations, min_improvement, patience, metadata, verbose)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Generate documentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìù Generating feature documentation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mfeature_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_documentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_iteration_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# Print final results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2955397916.py\u001b[0m in \u001b[0;36m_generate_documentation\u001b[0;34m(self, best_iteration_data)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# Add explainability summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_iteration_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mscores_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_scores_from_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FeatureEngineeringPipeline' object has no attribute '_safe_scores_from_metadata'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wfdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMSbtCRP_HuF",
        "outputId": "f2efae44-3f75-489e-8fca-8698456e9fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.2)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2025.10.5)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3 wfdb-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wfdb\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def load_chapman_shaoxing_ecg(data_dir='chapman_ecg_data',\n",
        "                               max_samples=1000,\n",
        "                               extract_features=True):\n",
        "    \"\"\"\n",
        "    Load Chapman-Shaoxing 12-lead ECG Database\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing the dataset\n",
        "        max_samples: Maximum number of records to load (dataset is large)\n",
        "        extract_features: If True, extract statistical features from signals\n",
        "\n",
        "    Returns:\n",
        "        df: DataFrame with features and target\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Loading Chapman-Shaoxing ECG Database...\")\n",
        "\n",
        "    # If not downloaded, provide instructions\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"\\n‚ö†Ô∏è  Dataset not found at {data_dir}\")\n",
        "        print(\"\\nTo download the Chapman-Shaoxing database:\")\n",
        "        print(\"1. Install wfdb: pip install wfdb\")\n",
        "        print(\"2. Download using:\")\n",
        "        print(\"   import wfdb\")\n",
        "        print(\"   wfdb.dl_database('chapman', dl_dir='chapman_ecg_data')\")\n",
        "        print(\"\\nOr download from: https://physionet.org/content/ecg-arrhythmia/1.0.0/\")\n",
        "        return None\n",
        "\n",
        "    # Load the database records\n",
        "    records = []\n",
        "    labels = []\n",
        "\n",
        "    # Get list of record files\n",
        "    record_files = list(Path(data_dir).glob('*.hea'))\n",
        "\n",
        "    print(f\"Found {len(record_files)} records\")\n",
        "    print(f\"Loading up to {max_samples} samples...\")\n",
        "\n",
        "    for i, record_file in enumerate(record_files[:max_samples]):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"  Processed {i}/{min(max_samples, len(record_files))} records...\")\n",
        "\n",
        "        try:\n",
        "            # Get record name without extension\n",
        "            record_name = str(record_file).replace('.hea', '')\n",
        "\n",
        "            # Read the record\n",
        "            record = wfdb.rdrecord(record_name)\n",
        "\n",
        "            # Get the signals (12 leads)\n",
        "            signals = record.p_signal  # Shape: (n_samples, 12)\n",
        "\n",
        "            # Get diagnosis from comments\n",
        "            diagnosis = None\n",
        "            if hasattr(record, 'comments'):\n",
        "                for comment in record.comments:\n",
        "                    if 'Reason for admission' in comment or 'Diagnosis' in comment:\n",
        "                        diagnosis = comment\n",
        "                        break\n",
        "\n",
        "            # Extract features if requested\n",
        "            if extract_features:\n",
        "                features = extract_ecg_features(signals, record.sig_name)\n",
        "                records.append(features)\n",
        "            else:\n",
        "                # Just use raw signals (will be very large)\n",
        "                features = signals.flatten()\n",
        "                records.append(features)\n",
        "\n",
        "            labels.append(diagnosis if diagnosis else 'Unknown')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ö†Ô∏è  Error loading {record_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n‚úì Loaded {len(records)} records successfully\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    if extract_features:\n",
        "        df = pd.DataFrame(records)\n",
        "    else:\n",
        "        # For raw signals, create column names\n",
        "        n_features = len(records[0])\n",
        "        columns = [f'signal_{i}' for i in range(n_features)]\n",
        "        df = pd.DataFrame(records, columns=columns)\n",
        "\n",
        "    df['diagnosis'] = labels\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def extract_ecg_features(signals, lead_names):\n",
        "    \"\"\"\n",
        "    Extract statistical and clinical features from 12-lead ECG signals\n",
        "\n",
        "    Args:\n",
        "        signals: numpy array of shape (n_samples, 12) - the ECG signals\n",
        "        lead_names: list of lead names (e.g., ['I', 'II', 'III', 'aVR', ...])\n",
        "\n",
        "    Returns:\n",
        "        features: dict of extracted features\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Ensure we have 12 leads\n",
        "    if signals.shape[1] != 12:\n",
        "        print(f\"    ‚ö†Ô∏è  Expected 12 leads, got {signals.shape[1]}\")\n",
        "\n",
        "    # Extract features for each lead\n",
        "    for i, lead_name in enumerate(lead_names):\n",
        "        if i >= signals.shape[1]:\n",
        "            break\n",
        "\n",
        "        signal = signals[:, i]\n",
        "\n",
        "        # Basic statistical features\n",
        "        features[f'{lead_name}_mean'] = np.mean(signal)\n",
        "        features[f'{lead_name}_std'] = np.std(signal)\n",
        "        features[f'{lead_name}_min'] = np.min(signal)\n",
        "        features[f'{lead_name}_max'] = np.max(signal)\n",
        "        features[f'{lead_name}_range'] = np.max(signal) - np.min(signal)\n",
        "\n",
        "        # Percentiles\n",
        "        features[f'{lead_name}_p25'] = np.percentile(signal, 25)\n",
        "        features[f'{lead_name}_p50'] = np.percentile(signal, 50)\n",
        "        features[f'{lead_name}_p75'] = np.percentile(signal, 75)\n",
        "\n",
        "        # Advanced features\n",
        "        features[f'{lead_name}_skewness'] = calculate_skewness(signal)\n",
        "        features[f'{lead_name}_kurtosis'] = calculate_kurtosis(signal)\n",
        "        features[f'{lead_name}_energy'] = np.sum(signal ** 2)\n",
        "        features[f'{lead_name}_rms'] = np.sqrt(np.mean(signal ** 2))\n",
        "\n",
        "        # Zero crossing rate\n",
        "        features[f'{lead_name}_zcr'] = np.sum(np.diff(np.sign(signal)) != 0) / len(signal)\n",
        "\n",
        "    # Cross-lead features\n",
        "    # Standard limb leads: I, II, III\n",
        "    if len(lead_names) >= 3:\n",
        "        features['limb_correlation_I_II'] = np.corrcoef(signals[:, 0], signals[:, 1])[0, 1]\n",
        "        features['limb_correlation_I_III'] = np.corrcoef(signals[:, 0], signals[:, 2])[0, 1]\n",
        "        features['limb_correlation_II_III'] = np.corrcoef(signals[:, 1], signals[:, 2])[0, 1]\n",
        "\n",
        "    # Precordial leads correlation (V1-V6, typically leads 6-11)\n",
        "    if signals.shape[1] >= 12:\n",
        "        precordial_signals = signals[:, 6:12]\n",
        "        features['precordial_mean_correlation'] = np.mean([\n",
        "            np.corrcoef(precordial_signals[:, i], precordial_signals[:, j])[0, 1]\n",
        "            for i in range(6) for j in range(i+1, 6)\n",
        "        ])\n",
        "\n",
        "    # Overall heart activity\n",
        "    features['overall_mean_amplitude'] = np.mean(np.abs(signals))\n",
        "    features['overall_max_amplitude'] = np.max(np.abs(signals))\n",
        "    features['overall_variance'] = np.var(signals)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def calculate_skewness(signal):\n",
        "    \"\"\"Calculate skewness of signal\"\"\"\n",
        "    mean = np.mean(signal)\n",
        "    std = np.std(signal)\n",
        "    if std == 0:\n",
        "        return 0\n",
        "    return np.mean(((signal - mean) / std) ** 3)\n",
        "\n",
        "\n",
        "def calculate_kurtosis(signal):\n",
        "    \"\"\"Calculate kurtosis of signal\"\"\"\n",
        "    mean = np.mean(signal)\n",
        "    std = np.std(signal)\n",
        "    if std == 0:\n",
        "        return 0\n",
        "    return np.mean(((signal - mean) / std) ** 4) - 3\n",
        "\n",
        "\n",
        "def prepare_ecg_for_pipeline(df, target_type='binary'):\n",
        "    \"\"\"\n",
        "    Prepare ECG data for the feature engineering pipeline\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame from load_chapman_shaoxing_ecg()\n",
        "        target_type: 'binary' (normal vs abnormal) or 'multiclass' (specific diagnoses)\n",
        "\n",
        "    Returns:\n",
        "        df_prepared: DataFrame ready for pipeline\n",
        "        target_col: Name of target column\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nüìã Preparing ECG data for pipeline...\")\n",
        "\n",
        "    df_prepared = df.copy()\n",
        "\n",
        "    # Process diagnosis column\n",
        "    if target_type == 'binary':\n",
        "        # Create binary target: Normal vs Abnormal\n",
        "        # You'll need to define what \"Normal\" means based on the dataset\n",
        "        df_prepared['target'] = df_prepared['diagnosis'].apply(\n",
        "            lambda x: 0 if 'normal' in str(x).lower() or 'Unknown' in str(x) else 1\n",
        "        )\n",
        "        target_col = 'target'\n",
        "        print(f\"  Created binary target: {df_prepared['target'].value_counts().to_dict()}\")\n",
        "\n",
        "    elif target_type == 'multiclass':\n",
        "        # Use diagnosis categories directly\n",
        "        # Clean up diagnosis labels\n",
        "        df_prepared['target'] = df_prepared['diagnosis'].fillna('Unknown')\n",
        "\n",
        "        # Encode to numeric\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        df_prepared['target'] = le.fit_transform(df_prepared['target'])\n",
        "\n",
        "        target_col = 'target'\n",
        "        print(f\"  Created multiclass target with {df_prepared['target'].nunique()} classes\")\n",
        "        print(f\"  Class distribution:\\n{df_prepared['target'].value_counts()}\")\n",
        "\n",
        "    # Drop the original diagnosis column\n",
        "    df_prepared = df_prepared.drop(columns=['diagnosis'])\n",
        "\n",
        "    # Remove any rows with missing values\n",
        "    initial_rows = len(df_prepared)\n",
        "    df_prepared = df_prepared.dropna()\n",
        "    if len(df_prepared) < initial_rows:\n",
        "        print(f\"  ‚ö†Ô∏è  Dropped {initial_rows - len(df_prepared)} rows with missing values\")\n",
        "\n",
        "    print(f\"  ‚úì Final dataset: {df_prepared.shape[0]} rows √ó {df_prepared.shape[1]-1} features\")\n",
        "\n",
        "    return df_prepared, target_col"
      ],
      "metadata": {
        "id": "GhzHs-BM_E_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. Load and prepare the ECG data\n",
        "df_ecg = load_chapman_shaoxing_ecg(\n",
        "    data_dir='chapman_ecg_data',\n",
        "    max_samples=500,  # Start with subset for testing\n",
        "    extract_features=True\n",
        ")\n",
        "\n",
        "if df_ecg is not None:\n",
        "    # 2. Prepare for pipeline\n",
        "    df_prepared, target_col = prepare_ecg_for_pipeline(\n",
        "        df_ecg,\n",
        "        target_type='binary'  # or 'multiclass'\n",
        "    )\n",
        "\n",
        "    # 3. Initialize pipeline\n",
        "    pipeline = FeatureEngineeringPipeline(\n",
        "        api_key=\"your-openai-key\",\n",
        "        serper_api_key=None,  # Optional\n",
        "        task_type='classification',\n",
        "        models=['random_forest', 'xgboost', 'gradient_boosting'],\n",
        "        min_explainability_score=3,\n",
        "        max_iterations=10,\n",
        "        patience=3\n",
        "    )\n",
        "\n",
        "    # 4. Add domain metadata for better results\n",
        "    metadata = {\n",
        "        'domain': 'healthcare - cardiology',\n",
        "        'problem': 'ECG arrhythmia detection from 12-lead signals',\n",
        "        'description': 'Predicting cardiac abnormalities from Chapman-Shaoxing ECG database with statistical features extracted from 12 leads (I, II, III, aVR, aVL, aVF, V1-V6)'\n",
        "    }\n",
        "\n",
        "    # 5. Run pipeline\n",
        "    result = pipeline.run(\n",
        "        df_prepared,\n",
        "        target_col=target_col,\n",
        "        max_iterations=15,\n",
        "        min_improvement=0.005,\n",
        "        patience=3,\n",
        "        metadata=metadata,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # 6. Save results\n",
        "    saved_files = pipeline.save_results(result, base_filename='ecg_arrhythmia')\n",
        "\n",
        "    # 7. Optional: Plot progression\n",
        "    pipeline.plot_progression()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üéâ ECG PIPELINE COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best Score: {result['best_score']:.4f}\")\n",
        "    print(f\"Best Features: {len(result['X_best'].columns)}\")\n",
        "    print(f\"\\nSaved files:\")\n",
        "    for key, path in saved_files.items():\n",
        "        print(f\"  - {key}: {path}\")"
      ],
      "metadata": {
        "id": "nExyZQuwVlFq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}